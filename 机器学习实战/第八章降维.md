# 维度的诅咒
这是嘛：高位空间距离会变远——数据呈稀疏状，低维预测高维，只会不可靠，方差很大
想增加数据集达到密度阙值，但是指数型的增加。或是此空间的算力达不到
# 降维的方法
为啥要降维：
    特征太多训练太慢方法难找
    一些特征对任务完全不重要，合并高度相关特征不损失太多信息
    降到23维可以可视化，有利于捕捉特征或是变得更可解释
投影：
    主要数据实际都处于更低维超平面，将所有实例垂直投影到该子空间
    比如瑞士卷，主成分子空间很多是扭曲的，无法处理
局部流形：
    有欲望就会有限制，所以其实是在将数据集压缩成低维流形
# PCA主成分分析
干嘛的：识别最靠近数据的超平面，然后投影
投哪个线或哪个面：均方距离最小的线或者面。十维会找到10条相互正交的主要成分轴
## 2.找主成分
SVD奇异值分解找主成分：
    矩阵自乘得方阵，特征值不变
实现：#Vt每行装一个零中心单位向量，s返回矩阵特征值，
    U ,s ,Vt = np.linalg.svd(X_centered)
## 3.向下投影
训练集矩阵\*前k大特征值对应的特征向量矩阵
## 4.实现
PCA(n_components=2)参数控制维数，数据自动居中
pca.components_.T获取主成分单位向量（对应的特征向量）
## 5.可解释方差比
方差代表信息量
pca.explained_variance_ratio_ 返回方差在各轴（主要成分）的比例
## 6.控制参数n_components以保留足够信息
不指定轴数（维度、主成分数
PCA(n_components=0.95)选择要95%的方差（信息
通过图找拐点的维数：绘制np.cumsum(pca.explained_variance_ratio_)
## 7.PCA压缩
逆变换回到原始维度：
    pca.fit_transform(X_train)
    pca.inverse_transform(X_reduced)
代码：绘制mnist数字
## 8.随机PCA
维度太多奇异值分解太慢，随机PCA算法可找到主成分近似值
PCA(n_components=154, svd_solver='randomized')，注：n>500且d<80%n自动转该算法
## 9.增量PCA
np.array_split(X_train, 100)返回100份批量
IncrementalPCA(n_components=154).partial_fit(X_batch)，引入np.memmap可配合成小部分fit
X_reduced = IncrementalPCA(n_components=154).transform(X_train)
# 内核PCA
干啥：高斯无限维核转换，在原空间作内积再转到z空间找边界。z空间易找的线性边界即原空间的超复杂边界。但是易overfit
    实例映射到无限维主成分会比较好找？？？然后再线性PCA投影
实现：
    KernelPCA(n_components=2, kernel='rbf', gamma=0.04).fit_transform(X)
    网格搜索哪个内核与最佳gamma值：
        param_grid = [{
        'kpca__gamma': np.linspace(0.03, 0.05, 10),
        'kpca__kernel': ['rbf', 'sigmoid']}]
        GridSearchCV(clf, param_grid, cv=3).fit(X,y)/.best_params_
    参数fit_inverse_transform=True，可使用inverse_transform()执行投影数据重构，从而计算与原始实例的误差
代码：
    获取并三维可视化瑞士卷数据ax.set_zlabel('')，数学符号gamma
# LLE
干嘛：不要投影，要流形学习
怎么搞：将每个实例重构为最近k个实例的线性函数，编码这些线性关系，映射到低维并尽可能多保留关系
    也就是该最近的还是最近的
实现：
    LocallyLinearEmbedding(n_components=2, n_neighbors=10).fit_transform(X)
# 随机投影
思想：
    你在第几维不重要，重要的是你带了多少人想降到几维
# t-SNE、LDA
相似的接近，不是一类的分开
学习各类的主成分并投影，因此不同类也会分开
作业10代码：
    各种渐变，各种渐变，各种标记绘制mnist散点图


```python
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import matplotlib as mpl
```


```python
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
```


```python
import os
PRD = '.'
CI = 'dim_reduction'
IP = os.path.join(PRD, 'images', CI)
os.makedirs(IP, exist_ok=True)
def save_fig(fig_id, tight_layout=True, fig_extension='png', res=300):
    path = os.path.join(IP, fig_id+'.'+fig_extension)
    print('Saving', fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format=fig_extension, dpi=res)
```


```python
# 手动实现PCA
np.random.seed(4)
m = 60
w1, w2 = 0.1, 0.3
noise = 0.1
# 60个01均匀分布，并作处理
angles = np.random.rand(m) *3 * np.pi/2 -0.5
# 60行3列零矩阵
X = np.empty((m, 3))
# 三列全装角度与噪声
X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) /2
X[:, 1] = np.sin(angles)*0.7 + noise * np.random.randn(m) /2
X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)
```


```python
# ******处理过的矩阵：减去均值的样本矩阵
X_centered = X - X.mean(axis=0)
# 奇艺值分解：Vt每行装一个零中心单位向量，s返回协方差矩阵的特征值？？？
U ,s ,Vt = np.linalg.svd(X_centered)
c1 = Vt.T[:, 0] # 只要第一二行
c2 = Vt.T[:, 1]

m, n = X.shape
S = np.zeros(X_centered.shape) # 60行3列0阵
# 前三行装入特征值矩阵
S[:n, :n] = np.diag(s)
np.allclose(X_centered, U.dot(S).dot(Vt)) # 1e-5误差内两两数组是否每一元素都相等
```




    True




```python
# 只要前两个特征值以及协方差矩阵的特征向量
W2 = Vt.T[:, :2]
# 投影到两特征向量的新矩阵
X2D = X_centered.dot(W2)
```


```python
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
# 此处X2D即投到两个特征向量后的矩阵
X2D = pca.fit_transform(X)
```


```python
X2D[:5]
```




    array([[ 1.26203346,  0.42067648],
           [-0.08001485, -0.35272239],
           [ 1.17545763,  0.36085729],
           [ 0.89305601, -0.30862856],
           [ 0.73016287, -0.25404049]])




```python
pca.explained_variance_ratio_
```




    array([0.84248607, 0.14631839])




```python
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split
X1, y = make_moons(n_samples=500, noise=0.3, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X1, y)
```


```python
from sklearn.datasets import fetch_openml
mnist = fetch_openml('mnist_784', version=1, as_frame=False)
mnist.target = mnist.target.astype(np.uint8)
X = mnist['data']
y = mnist['target']
X_train, X_test, y_train, y_test = train_test_split(X, y)
```


```python
pca = PCA()
pca.fit(X_train)
cumsum = np.cumsum(pca.explained_variance_ratio_)
d = np.argmax(cumsum >= 0.95) + 1
```


```python
pca = PCA(n_components=0.95)
X_reduced = pca.fit_transform(X_train)
```


```python
d
```




    154




```python
plt.figure(figsize=(6,4))
plt.plot(cumsum, linewidth=3)
plt.axis([0, 400, 0, 1])
plt.xlabel('D')
plt.ylabel('EV')
plt.plot([d,d], [0, 0.95], 'k:')
plt.plot([0,d], [0.95, 0.95], 'k:')
plt.plot(d, 0.95, 'ko')
plt.annotate('拐点', xy=(65, 0.85), xytext=(70, 0.7),
            arrowprops=dict(arrowstyle='->'), fontsize=16)
plt.grid(True)
save_fig('explained_variance_plot')
plt.show()
```

    Saving explained_variance_plot



    
![png](https://github.com/fendao/imgs/blob/main/ml第八章/output_15_1.png)
    



```python
pca = PCA(n_components=154)
X_reduced = pca.fit_transform(X_train)
X_recovered = pca.inverse_transform(X_reduced)
X_recovered.shape
```




    (52500, 784)




```python
def plot_digits(instances, images_per_row=5, **options):
    size=28
    # 确保每行数字数不超过实例总长度
    images_per_row = min(len(instances), images_per_row)
    # 实例总长度/每行数字数=总行数
    n_rows = (len(instances) - 1) // images_per_row + 1
    
    # 总数字数-实例数 = 空白数字数
    n_empty = n_rows * images_per_row - len(instances)
    # 实例和0矩阵纵向拼接
    padded_instances = np.concatenate([instances, np.zeros((n_empty, size*size))], axis=0)

    # 总行数 * 每行数字数 * 28 * 28 数组
    image_grid = padded_instances.reshape((n_rows, images_per_row, size, size))
    # 二维四维横向连接，一维三维纵向连接
    big_image = image_grid.transpose(0, 2, 1, 3).reshape(n_rows * size, images_per_row * size)
    plt.imshow(big_image, cmap=mpl.cm.binary, **options)
    plt.axis('off')
```


```python
plt.figure(figsize=(7,4))
plt.subplot(121)
plot_digits(X_train[::2100])
plt.title('初始', fontsize=16)
plt.subplot(122)
plot_digits(X_recovered[::2100])
plt.title('压缩', fontsize=16)
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第八章/output_18_0.png)
    



```python
X_recuced_pca = X_reduced
```


```python
rnd_pca = PCA(n_components=154, svd_solver='randomized')
X_reduced = rnd_pca.fit_transform(X_train)
```


```python
from sklearn.decomposition import IncrementalPCA
n_batches = 100
inc_pca = IncrementalPCA(n_components=154)
# 拆分为100份之后，批量训练数据partial_fit
for X_batch in np.array_split(X_train, n_batches):
    inc_pca.partial_fit(X_batch)
X_reduced = inc_pca.transform(X_train)
```


```python
X_recovered_inc_pca = inc_pca.inverse_transform(X_reduced)
```


```python
plt.figure(figsize=(7,4))
plt.subplot(121)
plot_digits(X_train[::2100])
plt.subplot(122)
plot_digits(X_recovered_inc_pca[::2100])
plt.tight_layout()
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第八章/output_23_0.png)
    



```python
filename='my_mnist.data'
m, n = X_train.shape
X_mm = np.memmap(filename, dtype='float32', mode='write', shape=(m, n))
X_mm[:] = X_train
# batch_size = m // n_batches
# inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)
# inc_pca.fit(X_mm)
```


```python
del X_mm
```


```python
X_mm = np.memmap(filename, dtype='float32', mode='readonly', shape=(m, n))
batch_size = m // n_batches
inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)
inc_pca.fit(X_mm)
```




    IncrementalPCA(batch_size=525, n_components=154)




```python
import time
for n_components in (2, 10, 154):
    print('n_components =', n_components)
    regular_pca = PCA(n_components=n_components, svd_solver='full')
    inc_pca = IncrementalPCA(n_components=n_components, batch_size=500)
    rnd_pca = PCA(n_components=n_components, random_state=42, svd_solver='randomized')
    for name, pca in (('PCA', regular_pca), ('Inc PCA', inc_pca), ('Rnd PCA', rnd_pca)):
        t1 = time.time()
        pca.fit(X_train)
        t2 = time.time()
        print('    {}: {:.1f} seconds'.format(name, t2-t1))
```

    n_components = 2
        PCA: 9.6 seconds
        Inc PCA: 30.1 seconds
        Rnd PCA: 0.9 seconds
    n_components = 10
        PCA: 9.1 seconds
        Inc PCA: 30.4 seconds
        Rnd PCA: 1.2 seconds
    n_components = 154
        PCA: 9.1 seconds
        Inc PCA: 41.0 seconds
        Rnd PCA: 4.2 seconds



```python
times_rpca = []
times_pca = []
sizes = [1000, 10000, 20000, 30000, 40000, 50000, 70000, 100000, 200000, 500000]
for n_samples in sizes:
    # 不同实例数量，特含固定五列
    X = np.random.randn(n_samples, 5)
    pca = PCA(n_components=2, svd_solver='randomized', random_state=42)
    t1 = time.time()
    pca.fit(X)
    t2 = time.time()
    # 不同实例量训练的时间用时
    times_rpca.append(t2-t1)
    pca = PCA(n_components=2, svd_solver='full')
    t1 = time.time()
    pca.fit(X)
    t2 = time.time()
    times_pca.append(t2-t1)
    
plt.plot(sizes, times_rpca, 'b-o', label='RPCA')
plt.plot(sizes, times_pca, 'r-s', label='PCA')
plt.xlabel('n_samples')
plt.ylabel('Training \ntime', rotation=0, labelpad=10)
plt.legend(loc='upper left')
plt.title('PCA与RPCA的时间复杂度')
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第八章/output_28_0.png)
    



```python
times_rpca = []
times_pca = []
sizes = [1000, 2000, 3000, 4000, 5000, 6000]
for n_features in sizes:
    X = np.random.randn(2000, n_features)
    pca = PCA(n_components=2, random_state=42, svd_solver='randomized')
    t1 = time.time()
    pca.fit(X)
    t2 = time.time()
    times_rpca.append(t2-t1)
    pca = PCA(n_components=2, svd_solver='full')
    t1 = time.time()
    pca.fit(X)
    t2 = time.time()
    times_pca.append(t2-t1)
    
plt.plot(sizes, times_rpca, 'k-o', label='RPCA')
plt.plot(sizes, times_pca, 'r-s', label='PCA')
plt.xlabel('n_features')
plt.ylabel('Training time')
plt.legend(loc='upper left')
plt.title('PCA与RPCA不同特征数时间复杂度')
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第八章/output_29_0.png)
    


# 内核PCA


```python
from sklearn.datasets import make_swiss_roll
X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)
from sklearn.decomposition import KernelPCA
rbf_pca = KernelPCA(n_components=2, kernel='rbf', gamma=0.04)
X_reduced = rbf_pca.fit_transform(X)
```


```python
lin_pca = KernelPCA(n_components=2, kernel='linear', fit_inverse_transform=True)
rbf_pca = KernelPCA(n_components=2, kernel='rbf', gamma=0.03, fit_inverse_transform=True)
sig_pca = KernelPCA(n_components=2, kernel='sigmoid', gamma=0.001, fit_inverse_transform=True)

y = t > 6.9
plt.figure(figsize=(11, 4))
# 不同的子图与模型做相同操作
for subplot, pca, title in ((131, lin_pca, 'Linear kernel'), (132, rbf_pca, 'RBF kernel, $\gamma=0.04$'), (133, sig_pca, 'Sigmoid kernel, $\gamma=10^{-3}, r=1$')):
    X_reduced = pca.fit_transform(X)
    if subplot == 132:
        X_reduced_rbf = X_reduced
    plt.subplot(subplot)
    plt.title(title, fontsize=14)
    plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.hot)
    plt.xlabel('$z_1$', fontsize=18)
    if subplot == 131:
        plt.ylabel('$z_2$', fontsize=18, rotation=0)
    plt.grid(True)
plt.show()

```


    
![png](https://github.com/fendao/imgs/blob/main/ml第八章/output_32_0.png)
    



```python
plt.figure(figsize=(6,5))
X_inverse = rbf_pca.inverse_transform(X_reduced_rbf)
ax = plt.subplot(111, projection='3d')
ax.view_init(10, -70)
ax.scatter(X_inverse[:, 0], X_inverse[:, 1], X_inverse[:, 2], c=t, cmap=plt.cm.hot, marker='x')
ax.set_xlabel('')
ax.set_ylabel('')
ax.set_zlabel('')
ax.set_xticklabels([])
ax.set_yticklabels([])
ax.set_zticklabels([])
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第八章/output_33_0.png)
    



```python
X_reduced = rbf_pca.fit_transform(X)
plt.figure(figsize=(11,4))
plt.subplot(132)
plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.hot, marker='x')
plt.xlabel('$z_1$', fontsize=18)
plt.ylabel('$z_2$', fontsize=18, rotation=0)
plt.grid(True)
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第八章/output_34_0.png)
    



```python
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
clf = Pipeline([
    ('kpca', KernelPCA(n_components=2)),
    ('log_reg', LogisticRegression(solver='lbfgs'))
])
param_grid = [{
        'kpca__gamma': np.linspace(0.03, 0.05, 10),
        'kpca__kernel': ['rbf', 'sigmoid']
    }]
grid_search = GridSearchCV(clf, param_grid, cv=3)
grid_search.fit(X, y)
```




    GridSearchCV(cv=3,
                 estimator=Pipeline(steps=[('kpca', KernelPCA(n_components=2)),
                                           ('log_reg', LogisticRegression())]),
                 param_grid=[{'kpca__gamma': array([0.03      , 0.03222222, 0.03444444, 0.03666667, 0.03888889,
           0.04111111, 0.04333333, 0.04555556, 0.04777778, 0.05      ]),
                              'kpca__kernel': ['rbf', 'sigmoid']}])




```python
print(grid_search.best_params_)
```

    {'kpca__gamma': 0.043333333333333335, 'kpca__kernel': 'rbf'}



```python
rbf_pca = KernelPCA(n_components=2, kernel='rbf', gamma=0.0433,
                   fit_inverse_transform=True)
X_reduced = rbf_pca.fit_transform(X)
X_preimage = rbf_pca.inverse_transform(X_reduced)
```


```python
from sklearn.metrics import mean_squared_error
mean_squared_error(X, X_preimage)
```




    32.78630879576614



# LLE


```python
from sklearn.manifold import LocallyLinearEmbedding
lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)
X_reduced = lle.fit_transform(X)
```


```python
plt.title('LLE平面数据点', fontsize=14)
plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.hot)
plt.xlabel('$z_1$', fontsize=18)
plt.ylabel('$z_2$', fontsize=18, rotation=0)
plt.axis([-0.065, 0.055, -0.1, 0.12])
plt.grid(True)
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第八章/output_41_0.png)
    



```python
from sklearn.manifold import MDS
mds = MDS(n_components=2, random_state=42)
X_reduced_mds = mds.fit_transform(X)
```


```python
from sklearn.manifold import Isomap
isomap = Isomap(n_components=2)
X_reduced_isomap = isomap.fit_transform(X)
```


```python
from sklearn.manifold import TSNE
tsne = TSNE(n_components=2, random_state=42)
X_reduced_tsne = tsne.fit_transform(X)
```

    /Users/zhangdi/opt/miniconda3/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.
      FutureWarning,
    /Users/zhangdi/opt/miniconda3/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.
      FutureWarning,



```python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
lda = LinearDiscriminantAnalysis(n_components=2)
X_mnist = mnist['data']
y_mnist = mnist['target']
lda.fit(X_mnist, y_mnist)
X_reduced_lda = lda.transform(X_mnist)
```


```python
titles = ['MDS', 'Isomap', 't_SNE']
plt.figure(figsize=(11,4))
for subplot, title, X_reduced in zip((131, 132, 133), titles, 
                                     (X_reduced_mds, X_reduced_isomap, X_reduced_tsne)):
    plt.subplot(subplot)
    plt.title(title, fontsize=14)
    plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.hot)
    plt.xlabel('$z_1$', fontsize=18)
    if subplot == 131:
        plt.ylabel('$z_2$', fontsize=18, rotation=0)
        plt.grid(True)
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第八章/output_46_0.png)
    



```python
# 练习9
X_train = mnist['data'][:60000]
y_train = mnist['target'][:60000]
X_test = mnist['data'][60000:]
y_test = mnist['target'][60000:]
```


```python
from sklearn.ensemble import RandomForestClassifier
rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)
```


```python
import time
t0 = time.time()
rnd_clf.fit(X_train, y_train)
t1 = time.time()
```


```python
print('took {:.2f}s'.format(t1-t0))
```

    took 26.73s



```python
from sklearn.metrics import accuracy_score
y_pred = rnd_clf.predict(X_test)
accuracy_score(y_test, y_pred)
```




    0.9705




```python
from sklearn.decomposition import PCA
pca = PCA(n_components=0.95)
X_train_reduced = pca.fit_transform(X_train)
```


```python
rnd_clf2 = RandomForestClassifier(n_estimators=100, random_state=42)
t0 = time.time()
rnd_clf2.fit(X_train_reduced, y_train)
t1 = time.time()
print('took {:.2f}s'.format(t1-t0))
```

    took 75.03s



```python
X_test_reduced = pca.transform(X_test)
y_pred = rnd_clf2.predict(X_test_reduced)
accuracy_score(y_test, y_pred)
```




    0.9481




```python
from sklearn.linear_model import LogisticRegression
log_clf = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=42)
t0 = time.time()
log_clf.fit(X_train, y_train)
t1 = time.time()
print('took {:.2f}s'.format(t1-t0))
```

    took 11.27s


    /Users/zhangdi/opt/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):
    STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
    
    Increase the number of iterations (max_iter) or scale the data as shown in:
        https://scikit-learn.org/stable/modules/preprocessing.html
    Please also refer to the documentation for alternative solver options:
        https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
      extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,



```python
y_pred = log_clf.predict(X_test)
accuracy_score(y_test, y_pred)
```




    0.9255




```python
log_clf2 = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=42)
t0 = time.time()
log_clf2.fit(X_train_reduced, y_train)
t1 = time.time()
```

    /Users/zhangdi/opt/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):
    STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
    
    Increase the number of iterations (max_iter) or scale the data as shown in:
        https://scikit-learn.org/stable/modules/preprocessing.html
    Please also refer to the documentation for alternative solver options:
        https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
      extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,



```python
y_pred = log_clf2.predict(X_test_reduced)
accuracy_score(y_test, y_pred)
```




    0.9201




```python
# 练习10
np.random.seed(42)
m = 10000
idx = np.random.permutation(60000)[:m]
X = mnist['data'][idx]
y = mnist['target'][idx]
```


```python
from sklearn.manifold import TSNE
tsne = TSNE(n_components=2, random_state=42)
X_reduced = tsne.fit_transform(X)
```

    /Users/zhangdi/opt/miniconda3/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.
      FutureWarning,
    /Users/zhangdi/opt/miniconda3/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.
      FutureWarning,



```python
plt.figure(figsize=(13,10))
plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, cmap='jet')
plt.axis('off')
plt.colorbar()
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第八章/output_61_0.png)
    



```python
plt.figure(figsize=(9,9))
cmap = mpl.cm.get_cmap('jet')
for digit in (2, 3, 5):
    plt.scatter(X_reduced[y == digit, 0], X_reduced[y == digit, 1], c=[cmap(digit / 9)])
    plt.axis('off')
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第八章/output_62_0.png)
    



```python
idx = (y==2) | (y==3) | (y==5)
X_subset = X[idx]
y_subset = y[idx]
tsne_subset = TSNE(n_components=2, random_state=42)
X_subset_reduced = tsne_subset.fit_transform(X_subset)
```

    /Users/zhangdi/opt/miniconda3/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.
      FutureWarning,
    /Users/zhangdi/opt/miniconda3/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.
      FutureWarning,



```python
plt.figure(figsize=(9,9))
for digit in (2,3,5):
    plt.scatter(X_subset_reduced[y_subset==digit, 0], X_subset_reduced[y_subset == digit, 1], c=[cmap(digit / 9)])
plt.axis('off')
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第八章/output_64_0.png)
    



```python
from sklearn.preprocessing import MinMaxScaler
from matplotlib.offsetbox import AnnotationBbox, OffsetImage
def plot_digits(X, y, min_distance=0.05, images=None, figsize=(13,10)):
    # 标准化特征
    X_normalized = MinMaxScaler().fit_transform(X)
    neighbors = np.array([[10., 10.]])
    plt.figure(figsize=figsize)
    cmap = mpl.cm.get_cmap('jet')
    digits = np.unique(y)
    for digit in digits:
        plt.scatter(X_normalized[y==digit, 0], X_normalized[y==digit, 1], c=[cmap(digit/9)])
    plt.axis('off')
    ax = plt.gcf().gca()
    for index, image_coord in enumerate(X_normalized):
        closest_distance = np.linalg.norm(neighbors - image_coord, axis=1).min()
        if closest_distance > min_distance:
            neighbors = np.r_[neighbors, [image_coord]]
            if images is None:
                plt.text(image_coord[0], image_coord[1], str(int(y[index])),
                        color=cmap(y[index]/9), fontdict={'weight':'bold', 'size':16})
            else:
                image = images[index].reshape(28, 28)
                imagebox = AnnotationBbox(OffsetImage(image, cmap='binary'), image_coord)
                ax.add_artist(imagebox)
```


```python
plot_digits(X_reduced, y)
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第八章/output_66_0.png)
    



```python
plot_digits(X_reduced, y, images=X, figsize=(35, 25))
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第八章/output_67_0.png)
    



```python
plot_digits(X_subset_reduced, y_subset, images=X_subset, figsize=(22, 22))
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第八章/output_68_0.png)
    



```python
from sklearn.decomposition import PCA
import time
t0 = time.time()
X_pca_reduced = PCA(n_components=2, random_state=42).fit_transform(X)
t1 = time.time()
print('took {:.1f}'.format(t1-t0))
plot_digits(X_pca_reduced, y)
plt.show()
```

    took 0.2



    
![png](https://github.com/fendao/imgs/blob/main/ml第八章/output_69_1.png)
    



```python
from sklearn.manifold import LocallyLinearEmbedding
t0 = time.time()
X_lle_reduced = LocallyLinearEmbedding(n_components=2, random_state=42).fit_transform(X)
t1 = time.time()
print('took {:.1f}s'.format(t1-t0))
plot_digits(X_lle_reduced, y)
plt.show()
```

    took 22.4s



    
![png](https://github.com/fendao/imgs/blob/main/ml第八章/output_70_1.png)
    



```python
from sklearn.pipeline import Pipeline
pca_lle = Pipeline([
    ('pca', PCA(n_components=0.95, random_state=42)),
    ('lle', LocallyLinearEmbedding(n_components=2, random_state=42))
])
t0 = time.time()
X_pca_lle_reduced = pca_lle.fit_transform(X)
t1 = time.time()
print('PCA+LLE took {:.1f}'.format(t1-t0))
plot_digits(X_pca_lle_reduced, y)
plt.show()
```

    PCA+LLE took 26.0



    
![png](https://github.com/fendao/imgs/blob/main/ml第八章/output_71_1.png)
    



```python
from sklearn.manifold import MDS
m = 2000
t0 = time.time()
X_mds_reduced = MDS(n_components=2, random_state=42).fit_transform(X[:m])
t1 = time.time()
print('MDS took {:.1f}'.format(t1-t0))
plot_digits(X_mds_reduced, y[:m])
plt.show()
```

    MDS took 32.2



    
![png](https://github.com/fendao/imgs/blob/main/ml第八章/output_72_1.png)
    



```python
from sklearn.pipeline import Pipeline
pca_mds = Pipeline([
    ('pca', PCA(n_components=0.95, random_state=42)),
    ('mds', MDS(n_components=2, random_state=42))
])
t0 = time.time()
X_pca_mds_reduced = pca_mds.fit_transform(X[:2000])
t1 = time.time()
print('PCA+MDS took {:.1f}s (on 2000 images)'.format(t1-t0))
plot_digits(X_pca_mds_reduced, y[:2000])
plt.show()
```

    PCA+MDS took 33.2s (on 2000 images)



    
![png](https://github.com/fendao/imgs/blob/main/ml第八章/output_73_1.png)
    



```python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
t0 = time.time()
X_lda_reduced = LinearDiscriminantAnalysis(n_components=2).fit_transform(X, y)
t1 = time.time()
print('LDS took {:.1f}s'.format(t1-t0))
plot_digits(X_lda_reduced, y, figsize=(12,12))
plt.show()
```

    LDS took 2.6s



    
![png](https://github.com/fendao/imgs/blob/main/ml第八章/output_74_1.png)
    



```python
from sklearn.manifold import TSNE
t0 = time.time()
X_tsne_reduced = TSNE(n_components=2, random_state=42).fit_transform(X)
t1 = time.time()
print('t-SNE took {:.1f}s'.format(t1-t0))
plot_digits(X_tsne_reduced, y)
plt.show()
```

    /Users/zhangdi/opt/miniconda3/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.
      FutureWarning,
    /Users/zhangdi/opt/miniconda3/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.
      FutureWarning,


    t-SNE took 23.4s



    
![png](https://github.com/fendao/imgs/blob/main/ml第八章/output_75_2.png)
    



```python
pca_tsne = Pipeline([
    ('pca', PCA(n_components=0.95, random_state=42)),
    ('tsne', TSNE(n_components=2, random_state=42))
])
t0 = time.time()
X_pca_tsne_reduced = pca_tsne.fit_transform(X)
t1 = time.time()
print('PCA+TSNE took {:.1f}s'.format(t1-t0))
plot_digits(X_pca_tsne_reduced, y)
plt.show()
```

    /Users/zhangdi/opt/miniconda3/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.
      FutureWarning,
    /Users/zhangdi/opt/miniconda3/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.
      FutureWarning,


    PCA+TSNE took 24.8s



    
![png](https://github.com/fendao/imgs/blob/main/ml第八章/output_76_2.png)
    

