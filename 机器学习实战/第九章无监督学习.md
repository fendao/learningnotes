聚类：相似的实例分到同集群。分析、细分、推荐、搜索、图像、降维
异常：学习正常检测异常。检测缺陷产品、时间序列的趋势
密度：低密度区域大概率异常。数据分析、可视化
# 聚类
不需要名字，只是根据相似性分配到同一集群
## 1.K-Means算法
干嘛的：
原始怎么干：
    kmeans_iter1随机初始化中心点，
    给实例整上与最近中心点所属集群标记kmeans_iter1.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)，
    在kmeans_iter1的决策边界，计算每个集群实例的均值重新定位中心点kmeans_iter2.cluster_centers_
    给实例整上标签kmeans_iter2.predict
    在2的边界，计算均值重做中心点kmeans_iter3.cluster_centers_
    标记数据，结束kmeans_iter3.predict
原始缺点：随机初始化中心点可能收敛到局部最优、利用距离分配集群时会因为实例直径不同可能分错
原始实现：
    KMeans(n_clusters=k).fit_predict(X)
    kmeans.labels_各实例所属集群的索引
    kmeans.cluster_centers_算法发现的k个中心点
    kmeans.transform(X)将数据转换为到各中心点的距离
改善中心点的初始化：
    法一：其他算法定位中心点，直接init=数组
    法二：设置参数(n_init=10)多次运行，留下最优。看
            kmeans.inertia_实例与中心点的均方距离
    法三：新K-Means自带——每次定位实例与中心点时都距离较远
小批量版本：
    MiniBatchKmeans(n_clusters=k)
选择最佳集群数k：
    根据惯性——由于集群数越多惯性就越低，因此也就只能选个惯性图的肘部
    根据轮廓分数(b-a)/max(a,b)， 越接近1，越相信a，也就是该实例位于此集群中且远离其他。0表示处于边界随时要走。-1即分配错
        silhouette_score(X, kmeans.labels_)计算
缺点：
    集群大小、密度不同表现不佳，利用距离定位集群依赖于集群呈现球形。椭圆就无从下手
聚类的用途——图像分割：
    干啥：同一类的像素分配给同一分割（集群
    怎么干：将3DRGB数组重构成一维，用KMeans聚类——找每种颜色集群的均色
    实现：
        对颜色进行聚类，分别246810个中心点,训练数据
            kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(X)
        labels是每个像素点(每行数据)预测的集群索引，并返回每个像素所属集群的中心点(中心像素点)，有533\*800个中心点
            segmented_img = kmeans.cluster_centers_[kmeans.labels_]
        加入高宽，重构成三维，这样才能imshow(接受二维平面，三维，RGBA)，segmented_imgs被装5个图片三维数据
            segmented_imgs.append(segmented_img.reshape(image.shape))
    代码：imread()加载图像、plt.imshow()呈现图像
聚类的用途——预处理数据
    作为预处理步骤找最佳集群数k：
        GridSearchCV(pipeline, dict(kmeans__n_clusters=range(2,100)), cv=3, verbose=2).fit(X_train, y_train)
        grid_clf.best_params_
    评估精度：grid_clf.score(X_test, y_test)
聚类的用途——半监督学习
    关键步骤找代表性实例：
        分50个集群，并获得实例到各中心点的距离数据
            X_digits_dist=KMeans(n_clusters=50).fit_transform(X_train)
        最靠近各中心点50个实例的索引
            representative_digit_idx=np.argmin(X_digits_dist, axis=0)
        训练集中找到这50个实例，作为代表性图像
            X_representative_digits=X_train[representative_digit_idx]
        看其所属集群，手动标记出50个标签，然后训练
        y_representative_digits = np.array([])
    标签传播到所有实例：
        y_train_propagated = np.empty(len(X_train), dtype=np.int32)
        for i in range(50);
            y_train_propagated[kmeans.labels_==i] = y_representative_digits[i]
    标签传播到50%中心点距离分位数的实例，为避免传播错误实例：
        每个实例到所属集群中心点的距离
            X_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]
            for i in range(50):
            第i集群所有实例索引（布尔数组
                in_cluster = (kmeans.labels_ == i)
            第i集群所有实例到中心点的距离
                cluster_dist = X_cluster_dist[in_cluster]
            第i集群50%距离分位数
                cutoff_distance = np.percentile(cluster_dist, 50)
            超过分位数实例的布尔索引，即淘汰实例
                above_cutoff = (X_cluster_dist > cutoff_distance)
            将淘汰实例的距离设置-1
                X_cluster_dist[in_cluster & above_cutoff] = -1
        非淘汰实例布尔索引
            partially_propagated = (X_cluster_dist != -1)
        所传播的更有精度的实例
            X_train_partially_propagated = X_train[partially_propagated]
            y_train_partially_propagated = y_train_propagated[partially_propagated]
代码：
    生成数据集make_blobs(n_features, n_samples, centers, cluster_std)
    plot_centroids圆圈黑叉标记中心点
    plot_decision_boundaries经典绘制决策边界
    两算法的性能与训练时长比较
    不同集群数的轮廓分数可视化与绘制轮廓图
## 2.DBSCAN算法
咋么干：计算各实例在ε邻域的实例数量，若有min_samples个，即定义为核心实例，其同属邻域的核心实例组成集群
        非核心实例且邻域内无实例的实例视为异常
实现：
    DBSCAN(eps=0.05, min_samples=5).fit(X)
    dbscan.labels_获取实例标签
    dbscan.core_sample_indices_获取核心实例索引
    dbscan.components_获取核心实例数据
解决DBSCAN无predict方法，利用KNN算法：
    KNeighborsClassifier(n_neighbors=50).fit(
        dbscan.components_, dbscan.labels_[dbscan.core_sample_indices])
        预测哪个集群——knn.predict(X)、估计每个集群的概率——knn.predict_proba(X)
解决DBSCAN不识别异常，kneighbors返回k个最近邻居的距离与索引，标记标签为-1：
    y_dist, y_pred_idx = knn.kneighbors(X_new, n_neighbors=1)
    y_pred = dbscan.labels_[dbscan.core_sample_indices_][y_pred_idx]
    y_pred[y_dist > 0.2] = -1
    y_pred.ravel()
代码：plot_dbscan
## 3.其他算法
聚集聚类（集群自己合并，得到二叉树
谱聚类（创建实例相似度矩阵并降维，在低维中KMeans聚类
# GMM高斯混合模型
干嘛的：聚类，且假定实例由多个高斯分布生成，所以集群椭圆状。只是大小密度和方向不同
GMM最简单变体：ψ⊃φ(j)集群权重——第j个集群的概率，实例的位置x～N( 均值μ(j), 协方差矩阵∑(j) )
            z(i)i实例的集群标记，来自ψ分布；z(i)定义μ、∑，从而得出x
实现：
    GaussianMixture(n_components=3, n_init=10).fit(X)
    gm.weights_获取ψ权重向量
    gm.means_获取μ
    gm.covariances_获取∑
    gm.converged_检查收敛性
    gm.n_iter_检查迭代次数
    gm.predict(X)硬聚类所属集群
    gm.predict_proba(X)软聚类集群概率
    X_new, y_new = gm.sample(10)生成10个新实例（按集群索引排序
    gm.score_samples(X)计算点位的PDF对数
降低任务难度（当维度多、集群多、实例少时）：
    GaussianMixture()的超参数covariance_type=
        full全形状、spherical球形集群、diag与坐标轴平行的椭圆、tied相同椭圆束
代码：
    经典绘制中心点、决策边界、密度
## 1.用高斯混合异常检测
干啥：定义阙值，检测低密度区域所有实例。能检测欺诈、缺陷、删除异常值
小于4%密度分位数即异常：
    debnsities = gm.score_samples(X)
    density_threshold = np.percentile(densities, 4)
    anomalies = X[density < density_threshold]
## 2.选择聚类数
高斯混合只能最小化信息准则模型BICAIC，而非惯性轮廓分：
    绘制信息准则-聚类数k图，观察最佳k
    网格搜索最佳covariance_type
代码：
    绘制函数、标题数学格式、虚线标记点、积分域标记点
    实现BIC、AIC计算
## 3.贝叶斯高斯混合
干啥：别手动搜索最佳k了，有自动实现。你想要10个也只会返回最佳的3个
怎么干：
实现：
    BayesianGaussianMixture(n_components=10, n_init=10).fit(X)
    np.round(bgm.weights_, 2)
    确定参数想知道结果X：weight_concentration_prior=0.01/10000
    知道结果X想确定参数的概率分布——后验。。。
缺点：拼命把集群搜成椭圆
代码：作业10——用next迭代抽样分布的索引数据而非for


```python
import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
import pandas as pd
```


```python
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
```


```python
import os
PRD = '.'
CI = 'dim_reduction'
IP = os.path.join(PRD, 'images', CI)
os.makedirs(IP, exist_ok=True)
def save_fig(fig_id, tight_layout=True, fig_extension='png', res=300):
    path = os.path.join(IP, fig_id+'.'+fig_extension)
    print('Saving', fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format=fig_extension, dpi=res)
```


```python
from matplotlib import pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import numpy as np

x = np.linspace(-3, 3, 50)
y = np.linspace(-3, 3, 50)
X, Y = np.meshgrid(x, y)
Z = X**2 + Y**2 
C=plt.contour(x, y,Z,[2,5,8,10])
plt.clabel(C, inline=True, fontsize=10)

fig=plt.figure()
fig = plt.figure(figsize=(10,10))
ax1 = plt.axes(projection='3d')

ax1.scatter3D(X,Y,Z, cmap='Blues')
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_4_0.png)
    



    <Figure size 640x480 with 0 Axes>



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_4_2.png)
    



```python
from sklearn.datasets import load_iris
data = load_iris()
X = data.data
y = data.target
data.target_names
```




    array(['setosa', 'versicolor', 'virginica'], dtype='<U10')




```python
plt.figure(figsize=(9, 3.5))
plt.subplot(121)
plt.plot(X[y==0, 2], X[y==0, 3], 'yo', label='山')
plt.plot(X[y==1, 2], X[y==1, 3], 'bs', label='多')
plt.plot(X[y==2, 2], X[y==2, 3], 'g^', label='弗')
plt.xlabel('长度', fontsize=14)
plt.ylabel('宽\n度', fontsize=14, rotation=0, labelpad=10)
plt.legend(fontsize=12)
plt.subplot(122)
plt.scatter(X[:, 2], X[:, 3], c='k', marker='.')
plt.xlabel('长度', fontsize=14)
plt.tick_params(labelleft=False)
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_6_0.png)
    



```python
from sklearn.mixture import GaussianMixture
y_pred = GaussianMixture(n_components=3, random_state=42).fit(X).predict(X)
```


```python
from scipy import stats
mapping = {}
for class_id in np.unique(y):
    mode, _ = stats.mode(y_pred[y==class_id])
    mapping[mode[0]] = class_id
mapping
```




    {1: 0, 2: 1, 0: 2}




```python
y_pred = np.array([mapping[cluster_id] for cluster_id in y_pred])
```


```python
plt.plot(X[y_pred==0, 2], X[y_pred==0, 3], 'yo', label='集群1')
plt.plot(X[y_pred==1, 2], X[y_pred==1, 3], 'bs', label='群2')
plt.plot(X[y_pred==2, 2], X[y_pred==2, 3], 'g^', label='群3')
plt.xlabel('花瓣长度', fontsize=14)
plt.ylabel('花瓣宽度', fontsize=14)
plt.legend(loc='upper left', fontsize=12)
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_10_0.png)
    



```python
np.sum(y_pred==y)
```




    145




```python
np.sum(y_pred==y) / len(y_pred)
```




    0.9666666666666667




```python
from sklearn.datasets import make_blobs
blob_centers = np.array(
    [[0.2, 2.3],
    [-1.5, 2.3],
    [-2.8, 1.8],
    [-2.8, 1.8],
    [-2.8, 1.3]])
blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])
```


```python
# 2000个实例，五种类别2000个标签数据
X, y = make_blobs(n_samples=2000, centers=blob_centers,
                 cluster_std=blob_std, random_state=7)
```


```python
def plot_clusters(X, y=None):
    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)
    plt.xlabel('$x_1$', fontsize=14)
    plt.ylabel('$x_2$', fontsize=14, rotation=0)
```


```python
plt.figure(figsize=(8, 4))
plot_clusters(X)
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_16_0.png)
    



```python
from sklearn.cluster import KMeans
k = 5
kmeans = KMeans(n_clusters=k, random_state=42)
y_pred = kmeans.fit_predict(X)
```


```python
y_pred
```




    array([3, 0, 4, ..., 0, 1, 0], dtype=int32)




```python
y_pred is kmeans.labels_
```




    True




```python
kmeans.cluster_centers_
```




    array([[-2.80258603,  1.80108628],
           [ 0.06154126,  2.58026834],
           [-1.47934812,  2.2874438 ],
           [-2.80006703,  1.30298291],
           [ 0.32780688,  1.98072917]])




```python
X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])
kmeans.predict(X_new)
```




    array([4, 4, 0, 0], dtype=int32)




```python
def plot_data(X):
    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)
def plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):
    if weights is not None:
        centroids = centroids[weights > weights.max() / 10]
    # 白圈加黑叉标记中心点，zorder绘图顺序大就晚
    plt.scatter(centroids[:, 0], centroids[:, 1],
               marker='o', s=35, linewidths=8,
               color=circle_color, zorder=10, alpha=0.9)
    plt.scatter(centroids[:, 0], centroids[:, 1],
               marker='x', s=2, linewidths=12,
               color=cross_color, zorder=11, alpha=1)
# 经典决策边界代码：最小最大值方便imshow，网格矩阵、Zvalue，等高线、轮廓填充
def plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,
                            show_xlabels=True, show_ylabels=True):
    mins = X.min(axis=0) - 0.1
    maxs = X.max(axis=0) + 0.1
    # 第1、2列最小最大间1000个点的xy坐标
    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),
                        np.linspace(mins[1], maxs[1], resolution))
    # 最1000个坐标点分别预测聚类，并变1000行
    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]), cmap='Pastel2')
    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]), linewidths=1, colors='k')
    plot_data(X)
    if show_centroids:
        plot_centroids(clusterer.cluster_centers_)
    if show_xlabels:
        plt.xlabel('$x_1$', fontsize=14)
    else:
        plt.tick_params(labelbottom=False)
    if show_ylabels:
        plt.ylabel('$x_2$', fontsize=14, rotation=0)
    else:
        plt.tick_params(labelleft=False)
```


```python
plt.figure(figsize=(8,4))
plot_decision_boundaries(kmeans, X)
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_23_0.png)
    



```python
kmeans.transform(X_new)
```




    array([[2.80963611, 0.58352264, 1.5070152 , 2.88551697, 0.32837283],
           [5.80599442, 2.99520469, 4.48856141, 5.84179855, 2.6722626 ],
           [1.21505819, 3.09017953, 1.67932083, 1.70875399, 3.48040395],
           [0.72625937, 3.06259333, 1.53543553, 1.21359924, 3.36807673]])




```python
np.linalg.norm(np.tile(X_new, (1, k)).reshape(-1, k, 2) - kmeans.cluster_centers_, axis=2)
```




    array([[2.80963611, 0.58352264, 1.5070152 , 2.88551697, 0.32837283],
           [5.80599442, 2.99520469, 4.48856141, 5.84179855, 2.6722626 ],
           [1.21505819, 3.09017953, 1.67932083, 1.70875399, 3.48040395],
           [0.72625937, 3.06259333, 1.53543553, 1.21359924, 3.36807673]])




```python
# 手撕kmeans随机中心点迭代算法
kmeans_iter1 = KMeans(n_clusters=5, init='random', n_init=1,
                     algorithm='full', max_iter=1, random_state=0)
kmeans_iter2 = KMeans(n_clusters=5, init='random', n_init=1,
                     algorithm='full', max_iter=2, random_state=0)
kmeans_iter3 = KMeans(n_clusters=5, init='random', n_init=1,
                     algorithm='full', max_iter=3, random_state=0)
kmeans_iter1.fit(X)
kmeans_iter2.fit(X)
kmeans_iter3.fit(X)
```




    KMeans(algorithm='full', init='random', max_iter=3, n_clusters=5, n_init=1,
           random_state=0)




```python
plt.figure(figsize=(10,8))
# 绘制数据、红圈白叉的随机初始中心点
plt.subplot(321)
plot_data(X)
plot_centroids(kmeans_iter1.cluster_centers_, circle_color='r', cross_color='w')
plt.ylabel('$x_2$', fontsize=14, rotation=0)
# 关闭x刻度
plt.tick_params(labelbottom=False)
plt.title('更新中心点（随机）', fontsize=14)

# 根据中心点给数据分配到集群做标记，绘制边界，关闭xy标签刻度
plt.subplot(322)
# show_centroids=True，白圈黑叉画中心点（第一次初始的）
plot_decision_boundaries(kmeans_iter1, X, show_xlabels=False, show_ylabels=False)
plt.title('标记实例', fontsize=14)

plt.subplot(323)
# y轴整上标签
plot_decision_boundaries(kmeans_iter1, X, show_centroids=False, show_xlabels=False)
# 使用第一次的标签计算各集群均值，重做中心点，白圈黑叉
plot_centroids(kmeans_iter2.cluster_centers_)

plt.subplot(324)
# 根据更新后的中心点利用距离重新标记实例，白圈黑叉更新后的中心点
plot_decision_boundaries(kmeans_iter2, X, show_xlabels=False, show_ylabels=False)

plt.subplot(325)
# xy都整上标签
plot_decision_boundaries(kmeans_iter2, X, show_centroids=False)
# 计算各集群标记数据的均值，重做中心点，白圈黑叉绘制
plot_centroids(kmeans_iter3.cluster_centers_)

plt.subplot(326)
# 用再新的中心点标记数据，白圈黑叉，x轴标签
plot_decision_boundaries(kmeans_iter3, X, show_ylabels=False)

plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_27_0.png)
    



```python
def plot_clusterer_comparison(clusterer1, clusterer2, X, title1=None, title2=None):
    clusterer1.fit(X)
    clusterer2.fit(X)
    plt.figure(figsize=(10, 3.2))
    plt.subplot(121)
    plot_decision_boundaries(clusterer1, X)
    if title1:
        plt.title(title1, fontsize=14)
    plt.subplot(122)
    plot_decision_boundaries(clusterer2, X, show_ylabels=False)
    if title2:
        plt.title(title2, fontsize=14)
```


```python
kmeans_rnd_init1 = KMeans(n_clusters=5, init='random', n_init=1,
                         algorithm='full', random_state=2)
kmeans_rnd_init2 = KMeans(n_clusters=5, init='random', n_init=1,
                         algorithm='full', random_state=5)
plot_clusterer_comparison(kmeans_rnd_init1, kmeans_rnd_init2, X,
                         '1', '2')
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_29_0.png)
    



```python
good_init = np.array([[-3,3], [-3,2], [-3, 1], [-1,2], [0,2]])
Kmeans = KMeans(n_clusters=5, init=good_init, n_init=1)
```


```python
Kmeans.fit(X)
Kmeans.inertia_
```




    186.58269923963434




```python
Kmeans.score(X)
```




    -186.5826992396343




```python
from sklearn.cluster import MiniBatchKMeans
minibatch_kmeans = MiniBatchKMeans(n_clusters=5)
minibatch_kmeans.fit(X)
```




    MiniBatchKMeans(n_clusters=5)




```python
minibatch_kmeans.inertia_
```




    172.3324221285333




```python
# 比较K-Means与MiniBatch
from timeit import timeit
times = np.empty((100,2))
inertias = np.empty((100,2))
for k in range(1, 101):
    kmeans_ = KMeans(n_clusters=k, random_state=42)
    minibatch_kmeans = MiniBatchKMeans(n_clusters=k, random_state=42)
    print('\r{}/{}'.format(k, 100), end='')
    # 运行10次（默认百万）返回最短的执行时间。第一列100个k用K-Means训练时长，第二列MiniBatch
    times[k-1, 0] = timeit('kmeans_.fit(X)', number=10, globals=globals())
    times[k-1, 1] = timeit('minibatch_kmeans.fit(X)', number=10, globals=globals())
    # 第一列100个k在KMeans算法所得惯性
    inertias[k-1, 0] = kmeans_.inertia_
    inertias[k-1, 1] = minibatch_kmeans.inertia_
```

    100/100


```python
plt.figure(figsize=(10,4))
plt.subplot(121)
plt.plot(range(1, 101), inertias[:, 0], 'r--', label='K-Means')
plt.plot(range(1, 101), inertias[:, 1], 'b.-', label='MiniBatchKMeans')
plt.xlabel('$k$', fontsize=15)
plt.title('惯性', fontsize=14)
plt.legend(fontsize=14)
plt.axis([1,100,0,100])

plt.subplot(122)
plt.plot(range(1,101), times[:, 0], 'r--', label='KMeans')
plt.plot(range(1,101), times[:, 1], 'b.-', label='MiniBatch')
plt.xlabel('$k$', fontsize=16)
plt.title('时长', fontsize=14)
plt.axis([1,100,0,6])
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_36_0.png)
    



```python
kmeans_k3 = KMeans(n_clusters=3, random_state=42)
kmeans_k8 = KMeans(n_clusters=8, random_state=42)
plot_clusterer_comparison(kmeans_k3, kmeans_k8, X, '$k=3$', '$k=8$')
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_37_0.png)
    



```python
kmeans_k3.inertia_
```




    278.81221249238814




```python
kmeans_k8.inertia_
```




    102.46641020829686




```python
kmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(X) for k in range(1, 10)]
inertias = [model.inertia_ for model in kmeans_per_k]
```


```python
plt.figure(figsize=(8, 3.5))
plt.plot(range(1,10), inertias, 'bo-')
plt.xlabel('$k$', fontsize=14)
plt.ylabel('Inertia', fontsize=14)
plt.annotate('Elbow', xy=(4, inertias[3]), xytext=(0.55, 0.55),
                         textcoords='figure fraction', fontsize=16,
                         arrowprops=dict(facecolor='black', shrink=0.1))
plt.axis([1, 8.5, 0, 1300])
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_41_0.png)
    



```python
plot_decision_boundaries(kmeans_per_k[4-1], X)
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_42_0.png)
    



```python
from sklearn.metrics import silhouette_score
silhouette_score(X, kmeans.labels_)
```




    0.5714611878168054




```python
silhouette_scores = [silhouette_score(X, model.labels_) for model in kmeans_per_k[1:]]
plt.figure(figsize=(8,3))
plt.plot(range(2,10), silhouette_scores, 'bo-')
plt.xlabel('$k$', fontsize=14)
plt.ylabel('轮廓分数', fontsize=14)
plt.axis([1.8, 8.5, 0.55, 0.7])
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_44_0.png)
    



```python
from sklearn.metrics import silhouette_samples
from matplotlib.ticker import FixedLocator, FixedFormatter
plt.figure(figsize=(11,9))
for k in (3,4,5,6):
    plt.subplot(2,2,k-2)
    y_pred = kmeans_per_k[k-1].labels_
    silhouette_coefficients = silhouette_samples(X, y_pred)
    padding = len(X) // 30
    pos = padding
    ticks = []
    for i in range(k):
        coeffs = silhouette_coefficients[y_pred == i]
        coeffs.sort()
        color = mpl.cm.Spectral(i/k)
        plt.fill_betweenx(np.arange(pos, pos+len(coeffs)), 0, coeffs,
                         facecolor=color, edgecolor=color, alpha=0.7)
        ticks.append(pos + len(coeffs)//2)
        pos += len(coeffs) + padding
    plt.gca().yaxis.set_major_locator(FixedLocator(ticks))
    plt.gca().yaxis.set_major_formatter(FixedFormatter(range(k)))
    if k in (3,5):
        plt.ylabel('集群')
    if k in (5,6):
        plt.gca().set_xticks([-0.1,0,0.2,0.4,0.6,0.8,1])
        plt.xlabel('轮廓系数')
    else:
        plt.tick_params(labelbottom=False)
    plt.axvline(x=silhouette_scores[k-2], color='red', linestyle='--')
    plt.title('$k={}$'.format(k), fontsize=16)
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_45_0.png)
    



```python
X1, y1 = make_blobs(n_samples=1000, centers=((4,-4), (0,0)), random_state=42)
X1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))
X2, y2 = make_blobs(n_samples=250, centers=1, random_state=42)
X2 = X2 + [6, -8]
X = np.r_[X1, X2]
y = np.r_[y1, y2]
```


```python
plot_clusters(X)
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_47_0.png)
    



```python
kmeans_good = KMeans(n_clusters=3, init=np.array([[-1.5, 2.5], [0.5, 0], [4,0]]), n_init=1, random_state=42)
kmeans_bad = KMeans(n_clusters=3, random_state=42)
kmeans_good.fit(X)
kmeans_bad.fit(X)
```




    KMeans(n_clusters=3, random_state=42)




```python
plt.figure(figsize=(10, 3.2))
plt.subplot(121)
plot_decision_boundaries(kmeans_good, X)
plt.title('惯性{:.1f}'.format(kmeans_good.inertia_), fontsize=14)
plt.subplot(122)
plot_decision_boundaries(kmeans_bad, X, show_ylabels=False)
plt.title('惯性{:.1f}'.format(kmeans_bad.inertia_), fontsize=14)
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_49_0.png)
    



```python
# import urllib.request
# images_path = os.path.join(PRD, 'images, unsupervised_learning')
# os.makedirs(images_path, exist_ok=True)
# DR = "https://raw.githubusercontent.com/ageron/handson-ml2/master/"
# filename = 'ladybug.png'
# print('Downloading', filename)
# url = DR + 'images/unsupervised_learning/' + filename
# urllib.request.urlretrieve(url, os.path.join(images_path, filename))
```


```python
from matplotlib.image import imread
image = imread(os.path.join('images', 'unsupervised_learning', 'ladybug.png'))
image.shape
```




    (533, 800, 3)



# 被聚类与Adaboost迷住
#虽然不知道发表这些总结有什么意义，有人更完善有何帮助，自己写点创意被一些人盗用又会挺烦
#为了展示只有用人单位或者想展示，前者必做，后者并非我也无需展示这


```python
# 一个三维有533个二维，每个二维有800个一维，每个一维有3个值
# 代表图片高533长800，每个像素点由RGB构成
# 重构，整成二维，只要像素，无问宽高
X = image.reshape(-1, 3)
kmeans = KMeans(n_clusters=8).fit(X)
segmented_img = kmeans.cluster_centers_[kmeans.labels_]
segmented_img = segmented_img.reshape(image.shape)
plt.imshow(segmented_img)
plt.axis('off')
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_53_0.png)
    



```python
segmented_imgs = []
n_colors = (10, 8, 6, 4, 2)
for n_clusters in n_colors:
    # 对颜色进行聚类，分别246810个中心点,训练数据
    kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(X)
    # labels是每个像素点(每行数据)预测的集群索引，并返回每个像素所属集群的中心点(中心像素点)，有533*800个中心点
    segmented_img = kmeans.cluster_centers_[kmeans.labels_]
    # 加入高宽，重构成三维，这样才能imshow(接受二维平面，三维，RGBA)，segmented_imgs被装5个图片三维数据
    segmented_imgs.append(segmented_img.reshape(image.shape))
```


```python
plt.figure(figsize=(10,5))
plt.subplots_adjust(wspace=0.05, hspace=0.1)
plt.subplot(231)
plt.imshow(image)
plt.title('原始')
plt.axis('off')
# 返回索引01234与像素集群中心点个数
for idx, n_clusters in enumerate(n_colors):
    plt.subplot(232 + idx)
    plt.imshow(segmented_imgs[idx])
    plt.title('像素中心点数{}'.format(n_clusters))
    plt.axis('off')
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_55_0.png)
    


# 预处理数据


```python
from sklearn.datasets import load_digits
X_digits, y_digits = load_digits(return_X_y=True)
```


```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits)
```


```python
log_reg = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=5000, random_state=42)
log_reg.fit(X_train, y_train)
```




    LogisticRegression(max_iter=5000, multi_class='ovr', random_state=42)




```python
log_reg.score(X_test, y_test)
```




    0.9466666666666667




```python
from sklearn.pipeline import Pipeline
pipeline = Pipeline([
    ('kmeans', KMeans(n_clusters=50)),
    ('log_reg', LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=5000, random_state=42))
])
pipeline.fit(X_train, y_train)
```




    Pipeline(steps=[('kmeans', KMeans(n_clusters=50)),
                    ('log_reg',
                     LogisticRegression(max_iter=5000, multi_class='ovr',
                                        random_state=42))])




```python
pipeline_score = pipeline.score(X_test, y_test)
pipeline_score
```




    0.9777777777777777




```python
from sklearn.model_selection import GridSearchCV
param_grid = dict(kmeans__n_clusters=range(2, 100))
grid_clf = GridSearchCV(pipeline, param_grid, cv=3, verbose=2)
grid_clf.fit(X_train, y_train)
```

    Fitting 3 folds for each of 98 candidates, totalling 294 fits
    [CV] END ...............................kmeans__n_clusters=2; total time=   0.1s
    ......
    [CV] END ..............................kmeans__n_clusters=99; total time=   2.3s





    GridSearchCV(cv=3,
                 estimator=Pipeline(steps=[('kmeans', KMeans(n_clusters=50)),
                                           ('log_reg',
                                            LogisticRegression(max_iter=5000,
                                                               multi_class='ovr',
                                                               random_state=42))]),
                 param_grid={'kmeans__n_clusters': range(2, 100)}, verbose=2)




```python
grid_clf.best_params_
```




    {'kmeans__n_clusters': 72}




```python
grid_clf.score(X_test, y_test)
```




    0.98



# 半监督


```python
# 半监督，给部分实例整上标签，再标签传播
n_labeled = 50
log_reg = LogisticRegression()
log_reg.fit(X_train[:n_labeled], y_train[:n_labeled])
```

   




    LogisticRegression()




```python
log_reg.score(X_test, y_test)
```




    0.8511111111111112




```python
k=50
kmeans = KMeans(n_clusters=k, random_state=42)
X_digits_dist = kmeans.fit_transform(X_train)
representative_digit_idx = np.argmin(X_digits_dist, axis=0)
X_representative_digits = X_train[representative_digit_idx]
```


```python
plt.figure(figsize=(8,2))
for index, X_representative_digit in enumerate(X_representative_digits):
    plt.subplot(k // 10, 10, index+1)
    plt.imshow(X_representative_digit.reshape(8, 8), cmap='binary', interpolation='bilinear')
    plt.axis('off')
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_70_0.png)
    



```python
y_train[representative_digit_idx]
```




    array([4, 3, 0, 7, 1, 5, 1, 3, 8, 8, 6, 1, 2, 4, 9, 1, 8, 7, 3, 5, 8, 6,
           3, 1, 1, 0, 7, 4, 3, 4, 2, 5, 1, 7, 2, 9, 9, 6, 4, 9, 0, 9, 7, 2,
           2, 6, 5, 6, 5, 9])




```python
y_representative_digits = np.array([
    3, 1, 4, 0, 5, 6, 7, 8, 1, 2,
    5, 7, 8, 1, 6, 3, 7, 6, 5, 2, 
    7, 4, 4, 1, 1, 9, 7, 2, 2, 5, 
    3, 4, 5, 6, 3, 6, 0, 1, 4, 0, 
    7, 9, 9, 3, 5, 2, 9, 5, 2, 8
])
```


```python
log_reg = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=5000, random_state=42)
log_reg.fit(X_representative_digits, y_representative_digits)
log_reg.score(X_test, y_test)
```




    0.08




```python
y_train_propagated = np.empty(len(X_train), dtype=np.int32)
for i in range(k):
    y_train_propagated[kmeans.labels_==i] = y_representative_digits[i]
```


```python
log_reg = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=5000, random_state=42)
log_reg.fit(X_train, y_train_propagated)
```




    LogisticRegression(max_iter=5000, multi_class='ovr', random_state=42)




```python
log_reg.score(X_test, y_test)
```




    0.07333333333333333




```python
percentile_closest = 75
X_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]
for i in range(k):
    in_cluster = (kmeans.labels_ == i)
    cluster_dist = X_cluster_dist[in_cluster]
    cutoff_distance = np.percentile(cluster_dist, percentile_closest)
    above_cutoff = (X_cluster_dist > cutoff_distance)
    X_cluster_dist[in_cluster & above_cutoff] = -1
```


```python
(X_cluster_dist != -1)
```




    array([False, False,  True, ...,  True, False,  True])




```python
partially_propagated = (X_cluster_dist != -1)
X_train_partially_propagated = X_train[partially_propagated]
y_train_partially_propagated = y_train_propagated[partially_propagated]
```


```python
log_reg = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=5000, random_state=42)
log_reg.fit(X_train_partially_propagated, y_train_partially_propagated)
```




    LogisticRegression(max_iter=5000, multi_class='ovr', random_state=42)




```python
log_reg.score(X_test, y_test)
```




    0.07777777777777778




```python
np.mean(y_train_partially_propagated == y_train[partially_propagated])
```




    0.09145129224652088



# DBSCAN


```python
from sklearn.datasets import make_moons
from sklearn.cluster import DBSCAN
X, y = make_moons(n_samples=1000, noise=0.05, random_state=42)
dbscan = DBSCAN(eps=0.05, min_samples=5)
dbscan.fit(X)
```




    DBSCAN(eps=0.05)




```python
dbscan.labels_[:10]
```




    array([ 0,  2, -1, -1,  1,  0,  0,  0,  2,  5])




```python
len(dbscan.core_sample_indices_)
```




    808




```python
dbscan.core_sample_indices_
```




    array([  0,   4,   5,   6,   7,   8,  10,  11,  12,  13,  14,  16,  17,
            18,  19,  20,  21,  22,  23,  24,  25,  26,  28,  29,  30,  31,
            32,  33,  34,  36,  38,  39,  41,  42,  44,  45,  47,  49,  50,
            51,  52,  53,  54,  55,  56,  58,  59,  61,  63,  64,  65,  66,
            67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,
            80,  81,  83,  84,  85,  87,  88,  89,  90,  91,  93,  94,  96,
            97,  98, 102, 103, 104, 105, 106, 107, 108, 109, 110, 113, 114,
           115, 116, 117, 119, 120, 122, 123, 124, 125, 126, 127, 128, 129,
           130, 135, 136, 139, 140, 141, 143, 144, 145, 146, 147, 148, 149,
           150, 152, 153, 154, 155, 156, 157, 158, 159, 161, 162, 163, 164,
           165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 177, 178,
           179, 181, 182, 183, 185, 186, 187, 188, 189, 191, 193, 194, 195,
           196, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210,
           211, 212, 213, 214, 215, 216, 217, 218, 219, 221, 222, 223, 224,
           226, 228, 229, 230, 232, 233, 234, 235, 236, 238, 239, 240, 241,
           242, 243, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 256,
           257, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271,
           272, 273, 274, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,
           287, 288, 289, 290, 291, 293, 294, 295, 296, 297, 300, 301, 303,
           304, 305, 308, 309, 310, 311, 313, 315, 317, 318, 319, 320, 321,
           322, 323, 324, 327, 328, 329, 330, 332, 333, 335, 339, 340, 341,
           342, 343, 346, 347, 348, 349, 351, 352, 353, 354, 355, 356, 358,
           360, 361, 362, 363, 364, 365, 366, 367, 368, 370, 371, 372, 373,
           374, 375, 377, 378, 379, 380, 381, 382, 384, 385, 387, 388, 389,
           390, 391, 392, 393, 394, 395, 397, 398, 399, 400, 401, 402, 403,
           404, 405, 406, 409, 411, 412, 413, 414, 415, 416, 417, 418, 419,
           420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432,
           433, 435, 436, 437, 438, 440, 441, 442, 443, 444, 445, 446, 447,
           448, 449, 450, 451, 452, 453, 454, 456, 457, 458, 459, 461, 462,
           463, 464, 467, 468, 469, 471, 472, 473, 474, 475, 476, 477, 478,
           479, 480, 483, 484, 485, 486, 487, 488, 489, 491, 492, 493, 495,
           496, 497, 498, 499, 501, 502, 503, 504, 505, 506, 507, 508, 509,
           510, 511, 512, 513, 514, 515, 516, 518, 519, 520, 521, 523, 524,
           525, 526, 528, 529, 530, 531, 532, 533, 535, 536, 537, 538, 539,
           540, 541, 542, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553,
           554, 555, 556, 557, 559, 560, 562, 563, 564, 565, 566, 568, 569,
           570, 572, 573, 574, 575, 576, 578, 579, 580, 583, 584, 585, 586,
           588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 599, 600, 601,
           602, 603, 604, 605, 607, 610, 611, 614, 615, 616, 617, 620, 624,
           625, 627, 628, 629, 631, 633, 634, 635, 636, 637, 638, 639, 640,
           641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 652, 655, 656,
           657, 661, 662, 663, 664, 666, 667, 670, 671, 672, 673, 675, 676,
           677, 678, 679, 680, 681, 682, 684, 685, 686, 688, 689, 690, 691,
           692, 694, 695, 696, 697, 698, 703, 704, 705, 706, 708, 709, 710,
           711, 712, 713, 714, 716, 717, 718, 719, 721, 722, 723, 724, 726,
           729, 730, 731, 733, 735, 736, 737, 738, 739, 740, 741, 742, 743,
           744, 745, 746, 748, 749, 750, 751, 752, 753, 754, 756, 757, 758,
           759, 760, 761, 762, 763, 765, 766, 768, 770, 772, 773, 774, 775,
           776, 777, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 791,
           792, 793, 794, 795, 796, 797, 798, 799, 800, 802, 803, 804, 805,
           806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818,
           819, 820, 821, 822, 824, 825, 826, 827, 828, 829, 830, 831, 832,
           835, 836, 837, 838, 839, 840, 841, 842, 843, 845, 846, 848, 849,
           850, 851, 852, 853, 854, 855, 857, 858, 860, 861, 862, 863, 864,
           865, 867, 868, 870, 871, 873, 877, 878, 879, 880, 882, 883, 884,
           885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897,
           898, 899, 902, 903, 904, 905, 906, 907, 908, 910, 912, 913, 916,
           918, 919, 920, 921, 922, 923, 925, 926, 928, 929, 930, 931, 932,
           933, 934, 935, 937, 938, 939, 940, 941, 942, 943, 944, 945, 947,
           948, 949, 951, 952, 953, 954, 956, 958, 959, 960, 961, 962, 963,
           964, 965, 966, 967, 968, 969, 970, 972, 974, 975, 976, 978, 979,
           980, 982, 983, 984, 985, 986, 987, 988, 990, 992, 993, 995, 997,
           998, 999])




```python
dbscan.components_
```




    array([[-0.02137124,  0.40618608],
           [-0.84192557,  0.53058695],
           [ 0.58930337, -0.32137599],
           ...,
           [ 1.66258462, -0.3079193 ],
           [-0.94355873,  0.3278936 ],
           [ 0.79419406,  0.60777171]])




```python
np.unique(dbscan.labels_)
```




    array([-1,  0,  1,  2,  3,  4,  5,  6])




```python
dbscan2 = DBSCAN(eps=0.2)
dbscan2.fit(X)
```




    DBSCAN(eps=0.2)




```python
def plot_dbscan(dbscan, X, size, show_xlabels=True, show_ylabels=True):
    # 1000个False
    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)
    # 核心实例为True
    core_mask[dbscan.core_sample_indices_] = True
    # 异常实例为True，1000个布尔，
    anomalies_mask = dbscan.labels_ == -1
    # 非异常非核心为True，，，非此且非彼操作！！！
    non_core_mask = ~(core_mask | anomalies_mask)
    
    # 核心实例坐标
    cores = dbscan.components_
    anomalies = X[anomalies_mask]    # 异常实例坐标
    non_cores = X[non_core_mask]    #非核非异常坐标
    
    plt.scatter(cores[:, 0], cores[:, 1], c=dbscan.labels_[core_mask],
               marker='o', s=size, cmap='Paired')
    plt.scatter(cores[:, 0], cores[:, 1], marker='*',
                s=20, c=dbscan.labels_[core_mask])
    # 红叉标记异常
    plt.scatter(anomalies[:, 0], anomalies[:, 1], c='r', marker='x', s=100)
    plt.scatter(non_cores[:, 0], non_cores[:, 1], c=dbscan.labels_[non_core_mask], marker='.')
    if show_xlabels:
        plt.xlabel('$x_1$', fontsize=14)
    else:
        plt.tick_params(labelbottom=False)
    if show_ylabels:
        plt.ylabel('$x_2$', fontsize=14, rotation=0)
    else:
        plt.tick_params(labelleft=False)
    plt.title('eps={:.2f}, min_samples={}'.format(dbscan.eps, dbscan.min_samples), fontsize=14)
```


```python
plt.figure(figsize=(9, 3.2))
plt.subplot(121)
plot_dbscan(dbscan, X, size=100)
plt.subplot(122)
plot_dbscan(dbscan2, X, size=600, show_ylabels=False)
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_92_0.png)
    



```python
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=50)
knn.fit(dbscan.components_, dbscan.labels_[dbscan.core_sample_indices_])
```




    KNeighborsClassifier(n_neighbors=50)




```python
X_new = np.array([[-0.5, 0], [0,0.5], [1,-0.1], [2,1]])
knn.predict(X_new)
```




    array([6, 0, 3, 2])




```python
knn.predict_proba(X_new)
```




    array([[0.24, 0.  , 0.  , 0.  , 0.  , 0.  , 0.76],
           [1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],
           [0.  , 0.  , 0.3 , 0.7 , 0.  , 0.  , 0.  ],
           [0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  ]])




```python
plt.figure(figsize=(6,3))
plot_decision_boundaries(knn, X, show_centroids=False)
plt.scatter(X_new[:, 0], X_new[:, 1], c='b', marker='+', s=200, zorder=10)
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_96_0.png)
    



```python
y_dist, y_pred_idx = knn.kneighbors(X_new, n_neighbors=1)
y_pred = dbscan.labels_[dbscan.core_sample_indices_][y_pred_idx]
y_pred[y_dist > 0.2] = -1
y_pred.ravel()
```




    array([-1,  0,  3, -1])



# 其他聚类


```python
from sklearn.cluster import SpectralClustering
sc1 = SpectralClustering(n_clusters=2, gamma=100, random_state=42)
sc1.fit(X)
```




    SpectralClustering(gamma=100, n_clusters=2, random_state=42)




```python
sc2 = SpectralClustering(n_clusters=2, gamma=1, random_state=42)
sc2.fit(X)
```




    SpectralClustering(gamma=1, n_clusters=2, random_state=42)




```python
np.percentile(sc1.affinity_matrix_, 95)
```




    0.04251990648936265




```python
def plot_spectral_clustering(sc, X, size, alpha, show_xlabels=True, show_ylabels=True):
    plt.scatter(X[:, 0], X[:, 1], marker='o', s=size, c='gray', cmap='Paired', alpha=alpha)
    plt.scatter(X[:, 0], X[:, 1], marker='o', s=30, c='w')
    plt.scatter(X[:, 0], X[:, 1], marker='.', s=10, c=sc.labels_, cmap='Paired')
    if show_xlabels:
        plt.xlabel('$x_1$', fontsize=14)
    else:
        plt.tick_params(labelbottom=False)
    if show_ylabels:
        plt.ylabel('$x_2$', fontsize=14, rotation=0)
    else:
        plt.tick_params(labelleft=False)
    plt.title('RBF gamma={}'.format(sc.gamma), fontsize=14)
```


```python
plt.figure(figsize=(9, 3.2))
plt.subplot(121)
plot_spectral_clustering(sc1, X, size=500, alpha=0.1)
plt.subplot(122)
plot_spectral_clustering(sc2, X, size=4000, alpha=0.01, show_ylabels=False)
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_103_0.png)
    



```python
from sklearn.cluster import AgglomerativeClustering
X = np.array([0,2,5,8.5]).reshape(-1, 1)
agg = AgglomerativeClustering(linkage='complete').fit(X)
```


```python
def learned_parameters(estimator):
    return [attrib for attrib in dir(estimator)
           if attrib.endswith('_') and not attrib.startswith('_')]
```


```python
learned_parameters(agg)
```




    ['children_',
     'labels_',
     'n_clusters_',
     'n_connected_components_',
     'n_features_in_',
     'n_leaves_']




```python
agg.children_
```




    array([[0, 1],
           [2, 3],
           [4, 5]])



# 高斯混合模型


```python
X1, y1 = make_blobs(n_samples=1000, centers=((4,-4), (0,0)), random_state=42)
X1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))
X2, y2 = make_blobs(n_samples=250, centers=1, random_state=42)
X2 = X2 + [6, -8]
X = np.r_[X1, X2]
y = np.r_[y1, y2]
```


```python
from sklearn.mixture import GaussianMixture
gm = GaussianMixture(n_components=3, n_init=10)
gm.fit(X)
```




    GaussianMixture(n_components=3, n_init=10)




```python
gm.weights_
```




    array([0.39025715, 0.40007391, 0.20966893])




```python
gm.means_
```




    array([[ 0.05131611,  0.07521837],
           [-1.40763156,  1.42708225],
           [ 3.39893794,  1.05928897]])




```python
gm.covariances_
```




    array([[[ 0.68799922,  0.79606357],
            [ 0.79606357,  1.21236106]],
    
           [[ 0.63479409,  0.72970799],
            [ 0.72970799,  1.1610351 ]],
    
           [[ 1.14833585, -0.03256179],
            [-0.03256179,  0.95490931]]])




```python
gm.converged_
```




    True




```python
gm.n_iter_
```




    4




```python
gm.predict(X)
```




    array([0, 0, 1, ..., 2, 2, 2])




```python
gm.predict_proba(X)
```




    array([[9.76741808e-01, 6.78581203e-07, 2.32575136e-02],
           [9.82832955e-01, 6.76173663e-04, 1.64908714e-02],
           [7.46494398e-05, 9.99923327e-01, 2.02398402e-06],
           ...,
           [4.26050456e-07, 2.15512941e-26, 9.99999574e-01],
           [5.04987704e-16, 1.48083217e-41, 1.00000000e+00],
           [2.24602826e-15, 8.11457779e-41, 1.00000000e+00]])




```python
X_new, y_new = gm.sample(6)
X_new
```




    array([[-0.37292812,  0.12420761],
           [ 0.77778732,  0.80315282],
           [ 2.06975207,  2.83792339],
           [-0.26709216, -0.67628326],
           [-0.27895205,  3.11578274],
           [ 4.03729104,  0.3497093 ]])




```python
y_new
```




    array([0, 0, 0, 0, 1, 2])




```python
gm.score_samples(X)
```




    array([-2.60768954, -3.57110232, -3.32987086, ..., -3.51347241,
           -4.39798588, -3.80746532])




```python
resolution = 100
# 自定义小网格边长，0.01间隔-10到9.99两千个值
grid = np.arange(-10, 10, 1/resolution)
xx, yy = np.meshgrid(grid, grid)
# 2行400万变400万两列，每行点坐标
X_full = np.vstack([xx.ravel(), yy.ravel()]).T

# 对每个点坐标估计概密分数
pdf = np.exp(gm.score_samples(X_full))
# 估计小网格生成实例的概率。小网格一角的概密 乘 小网格面积
pdf_probas= pdf * (1 / resolution) ** 2
pdf_probas.sum()
```




    0.9999999999215021




```python
from matplotlib.colors import LogNorm
def plot_gaussian_mixture(clusterer, X, resolution=1000, show_ylabels=True):
    # 数据都包含进去
    mins = X.min(axis=0) - 0.1
    maxs = X.max(axis=0) + 0.1
    # 根据自定义的网格数量切网格
    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),
                        np.linspace(mins[1], maxs[1], resolution))
    # 估计各点概密对数并取负
    Z = -clusterer.score_samples(np.c_[xx.ravel(), yy.ravel()])
    # 重组成1000行1000列，概密对数排成矩阵形式
    Z = Z.reshape(xx.shape)
    # np.logspace(start,end,num=50,endpoint=True,base=10,dtype=None)
    # 10的0次方start，10的2次end，幂指函数的中间12个值
    plt.contourf(xx, yy, Z,
                norm=LogNorm(vmin=1.0, vmax=30.0),
                levels=np.logspace(0, 2, 12))
    plt.contour(xx, yy, Z,
               norm=LogNorm(vmin=1.0, vmax=30.0),
                levels=np.logspace(0, 2, 12),
               linewidths=1, colors='k')
    # 各个点所估计的分配的集群索引
    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)    # 变矩阵形式
    plt.contour(xx, yy, Z,
               linewidths=2, colors='r', linestyles='dashed')
    # 用集群均值与集群权重就可绘出中心点。黑点描网格，黑叉描中心点
    plt.plot(X[:,0], X[:,1], 'k.', markersize=2)
    plot_centroids(clusterer.means_, clusterer.weights_)
    
    
    plt.xlabel('$x_1$', fontsize=14)
    if show_ylabels:
        plt.ylabel('$x_2$', fontsize=14, rotation=0)
    else:
        plt.tick_params(labelleft=False)
```


```python
plt.figure(figsize=(8,4))
plot_gaussian_mixture(gm,X)
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_123_0.png)
    



```python
gm_full = GaussianMixture(n_components=3, n_init=10, covariance_type='full', random_state=42)
gm_tied = GaussianMixture(n_components=3, n_init=10, covariance_type='tied', random_state=42)
gm_spherical = GaussianMixture(n_components=3, n_init=10, covariance_type='spherical', random_state=42)
gm_diag = GaussianMixture(n_components=3, n_init=10, covariance_type='diag', random_state=42)
gm_full.fit(X)
gm_tied.fit(X)
gm_spherical.fit(X)
gm_diag.fit(X)
```




    GaussianMixture(covariance_type='diag', n_components=3, n_init=10,
                    random_state=42)




```python
def compare_gaussian_mixtures(gm1, gm2, X):
    plt.figure(figsize=(9,4))
    plt.subplot(121)
    plot_gaussian_mixture(gm1, X)
    plt.title('{}'.format(gm1.covariance_type), fontsize=14)
    plt.subplot(122)
    plot_gaussian_mixture(gm2, X, show_ylabels=False)
    plt.title('{}'.format(gm2.covariance_type), fontsize=14)
```


```python
compare_gaussian_mixtures(gm_tied, gm_spherical, X)
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_126_0.png)
    



```python
compare_gaussian_mixtures(gm_full, gm_diag, X)
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_127_0.png)
    



```python
densities = gm.score_samples(X)
density_threshold = np.percentile(densities, 4)
anomalies = X[densities < density_threshold]
```


```python
plt.figure(figsize=(8,4))
plot_gaussian_mixture(gm, X)
plt.scatter(anomalies[:, 0], anomalies[:, 1], color='r', marker='*')
plt.ylim(top=5.1)
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_129_0.png)
    


# MLE、PDF、LOG估计


```python
from scipy.stats import norm
xx = np.linspace(-6, 4, 101)
ss = np.linspace(1, 2, 101)
XX, SS = np.meshgrid(xx, ss)
ZZ = 2 * norm.pdf(XX -1.0, 0, SS) + norm.pdf(XX + 4.0, 0, SS)
ZZ = ZZ / ZZ.sum(axis=1)[:, np.newaxis] / (xx[1] - xx[0])
```


```python
from matplotlib.patches import Polygon
plt.figure(figsize=(8, 4.5))
x_idx = 85
s_idx = 30
plt.subplot(221)
plt.contourf(XX, SS, ZZ, cmap='GnBu')
plt.plot([-6, 4], [ss[s_idx], ss[s_idx]], 'k-', linewidth=2)
plt.plot([xx[x_idx], xx[x_idx]], [1, 2], 'b-', linewidth=2)
plt.xlabel(r'$x$')
plt.ylabel(r'$\theta$', fontsize=14, rotation=0)
plt.title(r'Model $f(x; \theta)$', fontsize=14)

plt.subplot(222)
plt.plot(ss, ZZ[:, x_idx], 'b-')
max_idx = np.argmax(ZZ[:, x_idx])
max_val = np.max(ZZ[:, x_idx])
plt.plot(ss[max_idx], max_val, 'r.')
plt.plot([ss[max_idx], ss[max_idx]], [0, max_val], 'r:')
plt.plot([0, ss[max_idx]], [max_val, max_val], 'r:')
plt.text(1.01, max_val+0.005, r'$\hat{L}$', fontsize=14)
plt.text(ss[max_idx]+0.01, 0.055, r'$\hat{\theta}$', fontsize=14)
plt.text(ss[max_idx]+0.01, max_val-0.012, r'$Max$', fontsize=12)
plt.axis([1,2,0.05, 0.15])
plt.xlabel(r'$\theta$', fontsize=14)
plt.grid(True)
plt.text(1.99, 0.135, r'$=f(x=2.5; \theta)$', fontsize=14, ha='right')
plt.title(r'Likelihood function $\mathcal{L}(\theta|x=2.5)$', fontsize=14)

plt.subplot(223)
plt.plot(xx, ZZ[s_idx], 'k-')
plt.axis([-6, 4, 0, 0.25])
plt.xlabel(r'$x$', fontsize=14)
plt.grid(True)
plt.title(r'PDF $f(x; \theta=1.3)$', fontsize=14)
verts = [(xx[41], 0)] + list(zip(xx[41:81], ZZ[s_idx, 41:81])) + [(xx[80], 0)]
poly = Polygon(verts, facecolor='0.9', edgecolor='0.5')
plt.gca().add_patch(poly)

plt.subplot(224)
plt.plot(ss, np.log(ZZ[:, x_idx]), 'b-')
max_idx = np.argmax(np.log(ZZ[:, x_idx]))
max_val = np.max(np.log(ZZ[:, x_idx]))
plt.plot(ss[max_idx], max_val, 'r.')
plt.plot([ss[max_idx], ss[max_idx]], [-5, max_val], 'r:')
plt.plot([0, ss[max_idx]], [max_val, max_val], 'r:')
plt.axis([1, 2, -2.4, -2])
plt.xlabel(r'$\theta$', fontsize=14)
plt.text(ss[max_idx] + 0.01, max_val - 0.05, r'$Max$', fontsize=12)
plt.text(ss[max_idx] + 0.01, -2.39, r'$\hat{\theta}$', fontsize=14)
plt.text(1.01, max_val + 0.02, r'$\log \, \hat{L}$', fontsize=14)
plt.grid(True)
plt.title(r'$\log \, \mathcal{L}(\theta|x=2.5)$', fontsize=14)

save_fig("likelihood_function_plot")
plt.show()
```

    Saving likelihood_function_plot



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_132_1.png)
    



```python
gm.bic(X)
```




    8189.747000497186




```python
gm.aic(X)
```




    8102.521720382148




```python
n_clusters = 3
n_dims = 2
n_params_for_weights = n_clusters - 1
n_params_for_means = n_clusters * n_dims
n_params_for_covariance = n_clusters * n_dims * (n_dims + 1) // 2
n_params = n_params_for_weights + n_params_for_means + n_params_for_covariance
max_log_likelihood = gm.score(X) * len(X)
bic = np.log(len(X)) * n_params - 2 * max_log_likelihood
aic = 2 * n_params - 2 * max_log_likelihood
```


```python
bic, aic
```




    (8189.747000497186, 8102.521720382148)




```python
n_params
```




    17




```python
gms_per_k = [GaussianMixture(n_components=k, n_init=10, random_state=42).fit(X) for k in range(1, 11)]
bics = [model.bic(X) for model in gms_per_k]
aics = [model.aic(X) for model in gms_per_k]
```


```python
plt.figure(figsize=(8,3))
plt.plot(range(1,11), bics, 'bo-', label='BIC')
plt.plot(range(1,11), aics, 'go--', label='AIC')
plt.xlabel('$k$', fontsize=14)
plt.ylabel('信\n息\n量\n准\n则', fontsize=14, rotation=0, labelpad=10)
plt.axis([1, 9.5, np.min(aics)-50, np.max(aics)+50])
plt.annotate('最小值',
            xy=(3, bics[2]),
            xytext=(0.35, 0.6),
            textcoords='figure fraction',
            fontsize=14,
            arrowprops=dict(facecolor='black', shrink=0.1))
plt.legend()
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_139_0.png)
    



```python
min_bic = np.infty
for k in range(1,11):
    for covariance_type in ('full', 'tied', 'spherical', 'diag'):
        bic = GaussianMixture(n_components=k, n_init=10,
                             covariance_type=covariance_type,
                             random_state=42).fit(X).bic(X)
        if bic < min_bic:
            min_bic = bic
            best_k = k
            best_covariance_type = covariance_type
```


```python
best_k
```




    3




```python
best_covariance_type
```




    'full'



# 贝叶斯高斯混合模型???


```python
from sklearn.mixture import BayesianGaussianMixture
bgm = BayesianGaussianMixture(n_components=10, n_init=10, random_state=42)
bgm.fit(X)
np.round(bgm.weights_, 2)
```




    array([0.4 , 0.21, 0.4 , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ])




```python
plt.figure(figsize=(8,5))
plot_gaussian_mixture(bgm ,X)
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_145_0.png)
    



```python
bgm_low = BayesianGaussianMixture(n_components=10, max_iter=1000, n_init=1,
                                 weight_concentration_prior=0.01, random_state=42)
bgm_high = BayesianGaussianMixture(n_components=10, max_iter=1000, n_init=1,
                                  weight_concentration_prior=10000, random_state=42)
nn = 73
bgm_low.fit(X[:nn])
bgm_high.fit(X[:nn])
```




    BayesianGaussianMixture(max_iter=1000, n_components=10, random_state=42,
                            weight_concentration_prior=10000)




```python
np.round(bgm_low.weights_, 2)
```




    array([0.52, 0.48, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ])




```python
np.round(bgm_high.weights_, 2)
```




    array([0.01, 0.18, 0.27, 0.11, 0.01, 0.01, 0.01, 0.01, 0.37, 0.01])




```python
plt.figure(figsize=(9,4))
plt.subplot(121)
plot_gaussian_mixture(bgm_low, X[:nn])
plt.title('先验集中度0.01', fontsize=14)
plt.subplot(122)
plot_gaussian_mixture(bgm_high, X[:nn], show_ylabels=False)
plt.title('先验集中对10000', fontsize=14)
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_149_0.png)
    



```python
X_moons, y_moons = make_moons(n_samples=1000, noise=0.05, random_state=42)
bgm = BayesianGaussianMixture(n_components=10, n_init=10, random_state=42)
bgm.fit(X_moons)
```




    BayesianGaussianMixture(n_components=10, n_init=10, random_state=42)




```python
plt.figure(figsize=(9, 3.2))
plt.subplot(121)
plot_data(X_moons)
plt.xlabel('$x_1$', fontsize=14)
plt.ylabel('$x_2$', fontsize=14, rotation=0)
plt.subplot(122)
plot_gaussian_mixture(bgm, X_moons, show_ylabels=False)
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_151_0.png)
    


# 练习10
加载人脸集并分层抽样拆分、K-Means聚类并选择最佳集群数、可视化


```python
from sklearn.datasets import fetch_olivetti_faces
olivetti = fetch_olivetti_faces()
```

    downloading Olivetti faces from https://ndownloader.figshare.com/files/5976027 to /Users/zhangdi/scikit_learn_data



```python
print(olivetti.DESCR)
```

    .. _olivetti_faces_dataset:
    
    The Olivetti faces dataset
    --------------------------
    
    `This dataset contains a set of face images`_ taken between April 1992 and 
    April 1994 at AT&T Laboratories Cambridge. The
    :func:`sklearn.datasets.fetch_olivetti_faces` function is the data
    fetching / caching function that downloads the data
    archive from AT&T.
    
    .. _This dataset contains a set of face images: http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html
    
    As described on the original website:
    
        There are ten different images of each of 40 distinct subjects. For some
        subjects, the images were taken at different times, varying the lighting,
        facial expressions (open / closed eyes, smiling / not smiling) and facial
        details (glasses / no glasses). All the images were taken against a dark
        homogeneous background with the subjects in an upright, frontal position 
        (with tolerance for some side movement).
    
    **Data Set Characteristics:**
    
        =================   =====================
        Classes                                40
        Samples total                         400
        Dimensionality                       4096
        Features            real, between 0 and 1
        =================   =====================
    
    The image is quantized to 256 grey levels and stored as unsigned 8-bit 
    integers; the loader will convert these to floating point values on the 
    interval [0, 1], which are easier to work with for many algorithms.
    
    The "target" for this database is an integer from 0 to 39 indicating the
    identity of the person pictured; however, with only 10 examples per class, this
    relatively small dataset is more interesting from an unsupervised or
    semi-supervised perspective.
    
    The original dataset consisted of 92 x 112, while the version available here
    consists of 64x64 images.
    
    When using these images, please give credit to AT&T Laboratories Cambridge.
    



```python
olivetti.target
```




    array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,
            1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,
            3,  3,  3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  5,
            5,  5,  5,  5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,
            6,  6,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,
            8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,
           10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11,
           11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,
           13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15,
           15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,
           17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18,
           18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20,
           20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22,
           22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23,
           23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25,
           25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 27, 27,
           27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28, 28,
           28, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 30, 30, 30, 30, 30, 30,
           30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 32, 32, 32,
           32, 32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,
           34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 35, 35, 35, 35, 35, 35, 35,
           35, 35, 35, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 37, 37, 37, 37,
           37, 37, 37, 37, 37, 37, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 39,
           39, 39, 39, 39, 39, 39, 39, 39, 39])




```python
# next对列表等对象依次迭代，split对抽样分层实例按照Xy分数据，返回分开的索引并依次迭代？？？
# 按照索引分数据
from sklearn.model_selection import StratifiedShuffleSplit
strat_split = StratifiedShuffleSplit(n_splits=1, test_size=40,random_state=42)
train_valid_idx, test_idx = next(strat_split.split(olivetti.data, olivetti.target))
X_train_valid = olivetti.data[train_valid_idx]
y_train_valid = olivetti.target[train_valid_idx]
X_test = olivetti.data[test_idx]
y_test = olivetti.target[test_idx]

strat_split = StratifiedShuffleSplit(n_splits=1, test_size=80, random_state=42)
train_idx, valid_idx = next(strat_split.split(X_train_valid, y_train_valid))
X_train = X_train_valid[train_idx]
y_train = y_train_valid[train_idx]
X_valid = X_train_valid[valid_idx]
y_valid = y_train_valid[valid_idx]
```


```python
print(X_train.shape, y_train.shape)
print(X_valid.shape, y_valid.shape)
print(X_test.shape, y_test.shape)
```

    (280, 4096) (280,)
    (80, 4096) (80,)
    (40, 4096) (40,)



```python
from sklearn.decomposition import PCA
pca = PCA(0.99)
X_train_pca = pca.fit_transform(X_train)
X_valid_pca = pca.transform(X_valid)
X_test_pca = pca.transform(X_test)
pca.n_components_
```




    200




```python
from sklearn.cluster import KMeans
k_range = range(5, 150, 5)
# 装载不同集群数的KMeans模型
kmeans_per_k = []
for k in k_range:
    print('k={}'.format(k))
    kmeans = KMeans(n_clusters=k, random_state=42).fit(X_train_pca)
    kmeans_per_k.append(kmeans)
```

    k=5
    k=10
    k=15
    k=20
    k=25
    k=30
    k=35
    k=40
    k=45
    k=50
    k=55
    k=60
    k=65
    k=70
    k=75
    k=80
    k=85
    k=90
    k=95
    k=100
    k=105
    k=110
    k=115
    k=120
    k=125
    k=130
    k=135
    k=140
    k=145



```python
from sklearn.metrics import silhouette_score
silhouette_scores = [silhouette_score(X_train_pca, model.labels_) for model in kmeans_per_k]
best_index = np.argmax(silhouette_scores)
best_k = k_range[best_index]
best_score = silhouette_scores[best_index]

plt.figure(figsize=(8,3))
plt.plot(k_range, silhouette_scores, 'bo-')
plt.xlabel('$k$', fontsize=14)
plt.ylabel('Silhouette score', fontsize=14)
plt.plot(best_k, best_score, 'rs')
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_160_0.png)
    



```python
best_k
```




    110




```python
inertias = [model.inertia_ for model in kmeans_per_k]
best_inertia = inertias[best_index]

plt.figure(figsize=(8, 3.5))
plt.plot(k_range, inertias, 'bo-')
plt.xlabel('$k$', fontsize=14)
plt.ylabel('惯性', fontsize=14)
plt.plot(best_k, best_inertia, 'rs')
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_162_0.png)
    



```python
best_model = kmeans_per_k[best_index]
```


```python
def plot_faces(faces, labels, n_cols=5):
    faces = faces.reshape(-1, 64, 64)
    n_rows = (len(faces)-1) // n_cols + 1
    plt.figure(figsize=(n_cols, n_rows * 1.1))
    for index, (face, label) in enumerate(zip(faces, labels)):
        plt.subplot(n_rows, n_cols, index + 1)
        plt.imshow(face, cmap='gray')
        plt.axis('off')
        plt.title(label)
    plt.show()

for cluster_id in np.unique(best_model.labels_):
    print('Cluster', cluster_id)
    in_cluster = best_model.labels_==cluster_id
    faces = X_train[in_cluster]
    labels = y_train[in_cluster]
    plot_faces(faces, labels)
```

    Cluster 0



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_1.png)
    


    Cluster 1



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_3.png)
    


    Cluster 2



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_5.png)
    


    Cluster 3



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_7.png)
    


    Cluster 4



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_9.png)
    


    Cluster 5



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_11.png)
    


    Cluster 6



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_13.png)
    


    Cluster 7



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_15.png)
    


    Cluster 8



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_17.png)
    


    Cluster 9



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_19.png)
    


    Cluster 10



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_21.png)
    


    Cluster 11



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_23.png)
    


    Cluster 12



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_25.png)
    


    Cluster 13



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_27.png)
    


    Cluster 14



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_29.png)
    


    Cluster 15



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_31.png)
    


    Cluster 16



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_33.png)
    


    Cluster 17



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_35.png)
    


    Cluster 18



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_37.png)
    


    Cluster 19



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_39.png)
    


    Cluster 20



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_41.png)
    


    Cluster 21



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_43.png)
    


    Cluster 22



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_45.png)
    


    Cluster 23



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_47.png)
    


    Cluster 24



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_49.png)
    


    Cluster 25



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_51.png)
    


    Cluster 26



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_53.png)
    


    Cluster 27



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_55.png)
    


    Cluster 28



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_57.png)
    


    Cluster 29



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_59.png)
    


    Cluster 30



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_61.png)
    


    Cluster 31



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_63.png)
    


    Cluster 32



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_65.png)
    


    Cluster 33



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_67.png)
    


    Cluster 34



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_69.png)
    


    Cluster 35



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_71.png)
    


    Cluster 36



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_73.png)
    


    Cluster 37



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_75.png)
    


    Cluster 38



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_77.png)
    


    Cluster 39



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_79.png)
    


    Cluster 40



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_81.png)
    


    Cluster 41



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_83.png)
    


    Cluster 42



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_85.png)
    


    Cluster 43



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_87.png)
    


    Cluster 44



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_89.png)
    


    Cluster 45



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_91.png)
    


    Cluster 46



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_93.png)
    


    Cluster 47



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_95.png)
    


    Cluster 48



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_97.png)
    


    Cluster 49



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_99.png)
    


    Cluster 50



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_101.png)
    


    Cluster 51



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_103.png)
    


    Cluster 52



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_105.png)
    


    Cluster 53



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_107.png)
    


    Cluster 54



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_109.png)
    


    Cluster 55



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_111.png)
    


    Cluster 56



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_113.png)
    


    Cluster 57



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_115.png)
    


    Cluster 58



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_117.png)
    


    Cluster 59



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_119.png)
    


    Cluster 60



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_121.png)
    


    Cluster 61



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_123.png)
    


    Cluster 62



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_125.png)
    


    Cluster 63



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_127.png)
    


    Cluster 64



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_129.png)
    


    Cluster 65



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_131.png)
    


    Cluster 66



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_133.png)
    


    Cluster 67



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_135.png)
    


    Cluster 68



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_137.png)
    


    Cluster 69



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_139.png)
    


    Cluster 70



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_141.png)
    


    Cluster 71



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_143.png)
    


    Cluster 72



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_145.png)
    


    Cluster 73



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_147.png)
    


    Cluster 74



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_149.png)
    


    Cluster 75



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_151.png)
    


    Cluster 76



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_153.png)
    


    Cluster 77



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_155.png)
    


    Cluster 78



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_157.png)
    


    Cluster 79



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_159.png)
    


    Cluster 80



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_161.png)
    


    Cluster 81



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_163.png)
    


    Cluster 82



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_165.png)
    


    Cluster 83



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_167.png)
    


    Cluster 84



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_169.png)
    


    Cluster 85



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_171.png)
    


    Cluster 86



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_173.png)
    


    Cluster 87



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_175.png)
    


    Cluster 88



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_177.png)
    


    Cluster 89



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_179.png)
    


    Cluster 90



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_181.png)
    


    Cluster 91



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_183.png)
    


    Cluster 92



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_185.png)
    


    Cluster 93



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_187.png)
    


    Cluster 94



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_189.png)
    


    Cluster 95



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_191.png)
    


    Cluster 96



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_193.png)
    


    Cluster 97



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_195.png)
    


    Cluster 98



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_197.png)
    


    Cluster 99



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_199.png)
    


    Cluster 100



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_201.png)
    


    Cluster 101



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_203.png)
    


    Cluster 102



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_205.png)
    


    Cluster 103



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_207.png)
    


    Cluster 104



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_209.png)
    


    Cluster 105



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_211.png)
    


    Cluster 106



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_213.png)
    


    Cluster 107



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_215.png)
    


    Cluster 108



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_217.png)
    


    Cluster 109



    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_164_219.png)
    



```python
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(n_estimators=150, random_state=42)
clf.fit(X_train_pca, y_train)
clf.score(X_valid_pca, y_valid)
```




    0.9




```python
X_train_reduced = best_model.transform(X_train_pca)
X_valid_reduced = best_model.transform(X_valid_pca)
X_test_reduced = best_model.transform(X_test_pca)

clf = RandomForestClassifier(n_estimators=150, random_state=42)
clf.fit(X_train_reduced, y_train)
clf.score(X_valid_reduced, y_valid)
```




    0.7375




```python
from sklearn.pipeline import Pipeline
for n_clusters in k_range:
    pipeline = Pipeline([
        ('kmeans', KMeans(n_clusters=n_clusters, random_state=42)),
        ('fotest_clf', RandomForestClassifier(n_estimators=150, random_state=42))
    ])
    pipeline.fit(X_train_pca, y_train)
    print(n_clusters, pipeline.score(X_valid_pca, y_valid))
```

    5 0.45
    10 0.4625
    15 0.5375
    20 0.6125
    25 0.675
    30 0.675
    35 0.7
    40 0.6625
    45 0.675
    50 0.7
    55 0.675
    60 0.7
    65 0.725
    70 0.6875
    75 0.6875
    80 0.7375
    85 0.75
    90 0.7125
    95 0.725
    100 0.7125
    105 0.7
    110 0.7375
    115 0.7625
    120 0.75
    125 0.7
    130 0.7625
    135 0.75
    140 0.75
    145 0.75



```python
X_train_extened = np.c_[X_train_pca, X_train_reduced]
X_valid_extened = np.c_[X_valid_pca, X_valid_reduced]
X_test_extened = np.c_[X_test_pca, X_test_reduced]
```


```python
clf = RandomForestClassifier(n_estimators=150, random_state=42)
clf.fit(X_train_extened, y_train)
clf.score(X_valid_extened, y_valid)
```




    0.8



# 作业12
高斯混合、降维0.99、sample生成新面孔并可视化
修改图像翻转变暗、score_smaples检测异常


```python
from sklearn.mixture import GaussianMixture
gm = GaussianMixture(n_components=40, random_state=42)
y_pred = gm.fit_predict(X_train_pca)
```


```python
n_gen_faces = 20
gen_faces_reduced, y_gen_faces = gm.sample(n_samples=n_gen_faces)
gen_faces = pca.inverse_transform(gen_faces_reduced)
```


```python
plot_faces(gen_faces, y_gen_faces)
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_173_0.png)
    



```python
n_rotated = 4
rotated = np.transpose(X_train[:n_rotated].reshape(-1, 64, 64), axes=[0,2,1])
rotated = rotated.reshape(-1, 64*64)
y_rotated = y_train[:n_rotated]

n_flipped = 3
flipped = X_train[:n_flipped].reshape(-1, 64, 64)[:, ::-1]
flipped = flipped.reshape(-1, 64*64)
y_flipped = y_train[:n_flipped]

n_darkened = 3
darkened = X_train[:n_darkened].copy()
darkened[:, 1:-1] *= 0.3
y_darkened = y_train[:n_darkened]

X_bad_faces = np.r_[rotated, flipped, darkened]
y_bad = np.concatenate([y_rotated, y_flipped, y_darkened])

plot_faces(X_bad_faces, y_bad)
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_174_0.png)
    



```python
X_bad_faces_pca = pca.transform(X_bad_faces)
```


```python
gm.score_samples(X_bad_faces_pca)
```




    array([-5.12919321e+07, -3.15310623e+07, -2.95897643e+07, -4.73995991e+07,
           -2.57863659e+07, -5.03267284e+07, -5.07725129e+07, -1.10080828e+08,
           -8.83096726e+07, -8.80760801e+07])




```python
gm.score_samples(X_train_pca[:10])
```




    array([1168.94476547, 1118.66465801, 1118.66465606, 1129.3591842 ,
           1087.53034338, 1146.184557  , 1146.03684127, 1082.49224502,
           1168.65018095, 1129.35918291])



# 作业13
重建误差


```python
X_train_pca
```




    array([[ 2.2095535 ,  8.177854  , -3.1609113 , ...,  0.10564769,
             0.11277892, -0.12376244],
           [-2.8795416 , -5.8527203 , -3.1102674 , ..., -0.19481027,
            -0.07904657,  0.09704743],
           [-3.7210033 , -3.7166781 , -1.6135927 , ...,  0.10774743,
             0.06855951,  0.02083945],
           ...,
           [ 0.02803454, -1.4935057 ,  2.3138216 , ...,  0.12838449,
            -0.06186083,  0.01100544],
           [-2.0366478 , -2.057368  ,  1.7076209 , ..., -0.11621537,
            -0.04620285,  0.02861842],
           [ 4.334067  ,  0.7842247 , -1.6855737 , ...,  0.141804  ,
            -0.18896171, -0.02669648]], dtype=float32)




```python
def reconstruction_errors(pca, X):
    X_pca = pca.transform(X)
    X_reconstructed = pca.inverse_transform(X_pca)
    mse = np.square(X_reconstructed - X).mean(axis=1)
    return mse
```


```python
reconstruction_errors(pca, X_train).mean()
```




    0.00018892145




```python
reconstruction_errors(pca, X_bad_faces).mean()
```




    0.006214903




```python
plot_faces(X_bad_faces, y_bad)
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_183_0.png)
    



```python
X_bad_faces_reconstructed = pca.inverse_transform(X_bad_faces_pca)
plot_faces(X_bad_faces_reconstructed, y_pred)
```


    
![png](https://github.com/fendao/imgs/blob/main/ml第九章/output_184_0.png)
    

