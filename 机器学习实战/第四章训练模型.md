# 代码学习
散点画图方式、多线绘与一图、多次项式画图
轴刻度设置
定点给线、逻辑函数图绘制
定点标记、二元分类可视化、绘制文本与箭头
# 4.1线性回归
特征与偏置项的加权求和
目标：使方差成本函数最小θ
## 代码实现
标准方程：WLIN=θ=伪逆阵Xy
代码实现：
    np.linalg.inv计算伪逆阵，求得θ0、θ1
    求伪逆法二：np.linalg.pinv()直接得X伪逆
    再矩阵相乘得y_predict始终点y值
Scikit-Learn：
    实例、模型训练、intercept_coef_得θ0与θ1、predict得线
    法二：np.linalg.lstsq直接得θ0、θ1
# 4.2梯度下降
在初始化θ0每步降低成本函数MSE(θ)，直到趋向最小
目标：找使成本函数MSE(θ)最小化的参数组合Wlin
∨MSE(θ)成本函数的偏导（梯度）：指示四面八方上升最快的方向
学习率η步数
## 批量梯度下降
在每一步（迭代）中计算梯度∨MSE(θ)
代码实现：
## 随机梯度下降SGD
代码实现：
    每一步（迭代）随机找个实例计算梯度，每个迭代逐步降低学习率（步长η）
    学习率调度函数：eta = learning_schedule(epoch * m + i)
    确保实例iid：对实例混洗，使失去标签排序顺序
Scikit-Learn：
    SGDRegressor、训练模型、得偏置项与权重
## 小批量梯度下降
小批量、随机实例
# 4.3多项式回归
目标：线性模型拟合非线性D
介绍：多次项添加为新特征，使用新特征集训练模型
Scikit-Learn：
    PolynomialFeatures自动添加特征组合、fittransform添加二次项列的特征集、使用线性回归
# 4.4学习曲线
验证模型的泛化性能：交叉验证、学习曲线
原理：不同训练集大小上train和val集各自的RMSE水平
自定义绘制函数plot_learning_curves
# 4.5正则化
原理：约束模型权重θ减少次数，从而减少过拟合
## 岭回归？？
岭回归成本函数：添加了α/2l2范数进行约束
闭式解岭回归函数：
## Lasso回归？？
Lasso回归成本函数：添加αl1范数进行约束
## 弹性网路
加入混合比r与(1-r)分别控制双回归的惩罚项
ElasticNet模型
## 提前停止
Eval上升的时候即在过拟合，回滚模型参数到Eval最小的位置
算法实现：
# 4.6逻辑回归？？
对线性回归的预测值z二分化，嵌入sigmoid函数并取对数得到p、预测y
## 决策边界
用petal width特征检测是否弗吉尼亚：
    获取宽度数据X与0/1标签数据y、训练逻辑回归模型、制作x轴数据点并对对应点作各类估计分数、绘图
## Softmax回归
对某实例计算每个类k分数的指数，再归一化，分类器返回最高概率的一个类
Scikit-Learn：


```python
# 作业
a=0
while a<3:
    a+=1
    if a == 2:
        break
    print(a)
a=0
while a<3:
    a+=1
    if a == 2:
        continue
    print(a)
for i in range(4):
    for j in range(4):
            if j == 0:
                continue
                print(i)
            if j ==2:
                break
            print(i,j)
嵌套for循环中
continue：跳过后续代码进入下一次循环，该程序不运行内层第1次循环，内层直接运行第二次循环，所以输出的j不包含0
break：结束内层循环，所以内层只运行第二次，第3、4次循环均无输出
while循环中：
continue：结束本次，执行下一次循环
break：直接结束循环
当score输入在区间[90,100)时，index索引下标为3，返回第一个A
当输入为100时，若无第二个A，则返回为E
添加第二个A后，当score输入为100时，index索引下标为4，返回第二个A
```


      File "<ipython-input-394-239d5f113bd3>", line 23
        continue：跳过后续代码进入下一次循环，该程序不运行内层第1次循环，内层直接运行第二次循环，所以输出的j不包含0
                                                                  ^
    SyntaxError: invalid character in identifier




```python
import matplotlib as mpl
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```


```python
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
```


```python
X = 2 * np.random.rand(100, 1)
y = 4 + 3*X + np.random.randn(100, 1)
```


```python
from matplotlib.pyplot import MultipleLocator
```


```python
# plt.figure(figsize=(20,8), dpi=80)
plt.scatter(X, y)
plt.tick_params(axis='both',which='major',labelsize=14)
plt.xlabel('X1', fontsize=12)
plt.ylabel('y', fontsize=12)
# 刻度间隔
x_major_locator = MultipleLocator(0.25)
y_major_locator = MultipleLocator(2)
# 坐标轴实例
ax = plt.gca()
# 主刻度
ax.xaxis.set_major_locator(x_major_locator)
ax.yaxis.set_major_locator(y_major_locator)
# 刻度范围
plt.xlim(0, 2.)
plt.ylim(0, 15)
plt.show()
```


    
![](https://img-blog.csdnimg.cn/e1da4da7fc9a48a4a24a0face3f20708.png#pic_center)

    



```python
X_b = np.c_[np.ones((100, 1)), X]
X_b
```




    array([[1.        , 0.69056896],
           [1.        , 1.09503641],
           [1.        , 0.64677682],
           [1.        , 1.30597366],
           [1.        , 1.83011935],
           [1.        , 1.05779863],
           [1.        , 0.54006116],
           [1.        , 0.41063882],
           [1.        , 0.84434918],
           [1.        , 1.48040929],
           [1.        , 0.32755812],
           [1.        , 0.95007289],
           [1.        , 1.6222861 ],
           [1.        , 1.36803173],
           [1.        , 0.67995342],
           [1.        , 0.28375754],
           [1.        , 0.22192431],
           [1.        , 0.81670441],
           [1.        , 0.3227235 ],
           [1.        , 1.93555247],
           [1.        , 0.88910284],
           [1.        , 0.90670084],
           [1.        , 0.85993465],
           [1.        , 1.56791135],
           [1.        , 1.7641284 ],
           [1.        , 1.71865204],
           [1.        , 1.27361387],
           [1.        , 1.52356172],
           [1.        , 1.06240312],
           [1.        , 1.19423602],
           [1.        , 0.68403175],
           [1.        , 1.23269376],
           [1.        , 0.28656663],
           [1.        , 0.56283408],
           [1.        , 0.64074196],
           [1.        , 0.18367042],
           [1.        , 0.71660301],
           [1.        , 0.13905064],
           [1.        , 0.35977406],
           [1.        , 0.4753392 ],
           [1.        , 0.45272167],
           [1.        , 0.93375507],
           [1.        , 0.68137743],
           [1.        , 0.2406101 ],
           [1.        , 0.60453245],
           [1.        , 1.02911014],
           [1.        , 1.76223207],
           [1.        , 1.89428423],
           [1.        , 1.41578483],
           [1.        , 1.08903995],
           [1.        , 0.28713063],
           [1.        , 1.83932683],
           [1.        , 1.72162226],
           [1.        , 1.9239099 ],
           [1.        , 1.89466128],
           [1.        , 1.69328722],
           [1.        , 1.04791071],
           [1.        , 1.08904717],
           [1.        , 0.32126084],
           [1.        , 1.31982875],
           [1.        , 1.24665312],
           [1.        , 0.71878169],
           [1.        , 1.43133907],
           [1.        , 1.03540358],
           [1.        , 1.1733726 ],
           [1.        , 1.90103184],
           [1.        , 1.24935772],
           [1.        , 0.85959727],
           [1.        , 0.11931619],
           [1.        , 1.08489517],
           [1.        , 0.53089631],
           [1.        , 1.15935157],
           [1.        , 0.50477505],
           [1.        , 0.82253989],
           [1.        , 0.42160345],
           [1.        , 0.05328369],
           [1.        , 1.7194971 ],
           [1.        , 1.03510607],
           [1.        , 0.54010678],
           [1.        , 1.1045144 ],
           [1.        , 0.75630817],
           [1.        , 1.66230773],
           [1.        , 1.36801615],
           [1.        , 0.51904843],
           [1.        , 0.76730644],
           [1.        , 1.23956514],
           [1.        , 0.06545095],
           [1.        , 1.97346577],
           [1.        , 0.17622071],
           [1.        , 0.38396168],
           [1.        , 1.06051449],
           [1.        , 0.23115968],
           [1.        , 1.15153478],
           [1.        , 0.97422361],
           [1.        , 0.25063895],
           [1.        , 0.61187805],
           [1.        , 0.90840842],
           [1.        , 1.34972953],
           [1.        , 1.82988156],
           [1.        , 0.63734291]])




```python
theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)
```


```python
theta_best
```




    array([[4.07132793],
           [3.00060297]])




```python
X_new = np.array([[0], [2]])
X_new
```




    array([[0],
           [2]])




```python
X_new_b = np.c_[np.ones((2,1)), X_new]
y_predict = X_new_b.dot(theta_best)
y_predict
```




    array([[ 4.07132793],
           [10.07253387]])




```python
plt.plot(X_new, y_predict, 'r-')
plt.plot(X, y, 'b.')
plt.axis([0, 2, 0, 15])
plt.show()
```


    
    



```python
from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(X, y)
lin_reg.intercept_, lin_reg.coef_
```




    (array([4.07132793]), array([[3.00060297]]))




```python
lin_reg.predict(X_new)
```




    array([[ 4.07132793],
           [10.07253387]])




```python
theta_besta_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)
theta_besta_svd
```




    array([[4.07132793],
           [3.00060297]])




```python
np.linalg.pinv(X_b).dot(y)
```




    array([[4.07132793],
           [3.00060297]])




```python
count = 0
eta = 0.1
n_iterations = 1000
m =100
theta = np.random.randn(2,1)
xml = MultipleLocator(0.5)
yml = MultipleLocator(2)
fig, ax = plt.subplots()
for iteration in range(n_iterations):
    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)
    theta = theta - eta * gradients
#     theta3 = np.append(theta3, theta, axis=1)
#     if count < 10:
#     print(theta3)
    ax.plot(X, y,'b.')
    ax.plot(X_new, xx.dot(theta),'r-')
    plt.xlabel('X1', fontsize=12)
    plt.ylabel('y', fontsize=12)
    ax.xaxis.set_major_locator(xml)
    ax.yaxis.set_major_locator(yml)
    plt.xlim(0,2.)
    plt.ylim(0, 15)
    count += 1
plt.show()
```


    
![](https://img-blog.csdnimg.cn/7e83962d51af478cabe39923bb94ffec.png#pic_center)




```python
n_epochs = 100
t0, t1 = 5, 50
def learning_schedule(t):
    return t0 / (t + t1)
theta = np.random.randn(2,1)
xx = np.c_[np.ones((2,1)), np.array([[0],[2]])]
for epoch in range(n_epochs):
    for i in range(m):
        random_index = np.random.randint(m)
        xi = X_b[random_index:random_index+1]
        yi = y[random_index:random_index+1]
        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)
        eta = learning_schedule(epoch * m + i)
        theta = theta - eta * gradients
        plt.plot(X, y, 'b.')
        plt.plot(X_new, xx.dot(theta))
plt.show()
```


    
![在这里插入图片描述](https://img-blog.csdnimg.cn/00f2305e6a2c41278abbb5fbcb7d059c.png#pic_center)



```python
from sklearn.linear_model import SGDRegressor
sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)
sgd_reg.fit(X, y.ravel())
```




    SGDRegressor(eta0=0.1, penalty=None)




```python
sgd_reg.intercept_, sgd_reg.coef_
```




    (array([4.1182052]), array([3.1043404]))




```python
rrr = sgd_reg.predict(X_new)
```


```python
plt.plot(X,y, 'b.')
plt.plot(X_new, rrr, 'g-')
plt.show()
```


    
![在这里插入图片描述](https://img-blog.csdnimg.cn/ac9a725c8ec64f56bf5622fa5ace831e.png#pic_center)



```python
m = 100
X = 6 * np.random.rand(m ,1) - 3
y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)
plt.plot(X, y, 'b.')
plt.show()
```


    
![在这里插入图片描述](https://img-blog.csdnimg.cn/0575582aac7f4e80a47cf8f7903fa28f.png#pic_center)

    



```python
# 期待爬山、文言文翻译学习器（可以自己写点有趣的、可以开搞了）
from sklearn.preprocessing import PolynomialFeatures
poly_features = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly_features.fit_transform(X)
X[0]
```




    array([0.04875089])




```python
X_poly[0]
```




    array([0.04875089, 0.00237665])




```python
X_poly.shape
```




    (100, 2)




```python
lin_reg = LinearRegression()
lin_reg.fit(X_poly, y)
lin_reg.intercept_, lin_reg.coef_
```




    (array([2.05438764]), array([[0.96842795, 0.48549735]]))




```python
lin_reg2 = LinearRegression()
lin_reg2.fit(X, y)
lin_reg2.intercept_, lin_reg2.coef_
# 第二条直线
y_predict2 = lin_reg2.predict(np.array([[-3],[3]]))
```


```python
poly = PolynomialFeatures(degree=300,include_bias=False)
poly.fit(X)
X3 = poly.transform(X)
lin_reg3 = LinearRegression()
lin_reg3.fit(X3,y)
y_predict3 = lin_reg3.predict(X3)
# y_predict3.shape
X3.shape
```




    (100, 300)




```python
# 第一条曲线
y_predict = lin_reg.predict(X_poly)
y_predict
plt.plot(X, y, 'r.')
plt.plot(np.sort(X,axis=None), y_predict[np.argsort(X,axis=None)], 'g-')
plt.plot(np.sort(X,axis=None), y_predict3[np.argsort(X,axis=None)], 'k:')
plt.plot(np.array([[-3],[3]]), y_predict2, 'y--')
plt.show()
```


    
![在这里插入图片描述](https://img-blog.csdnimg.cn/dc9bf24d3f054ed79997895a4b577625.png#pic_center)

    



```python
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```


```python
def plot_learning_curves(model,X,y):
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)
    train_errors, val_errors = [], []
    for m in range(1, len(X_train)):
        model.fit(X_train[:m], y_train[:m])
        y_train_predict = model.predict(X_train[:m])
        y_val_predict = model.predict(X_val)
        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))
        val_errors.append(mean_squared_error(y_val, y_val_predict))
    plt.plot(np.sqrt(train_errors), 'r-+', linewidth=2, label='train')
    plt.plot(np.sqrt(val_errors), 'b-', linewidth=3, label='val')
    plt.legend()
    plt.xlabel('训练集大小', fontsize=12)
    plt.ylabel('RMSE', fontsize=12)
```


```python
lin_reg = LinearRegression()
plot_learning_curves(lin_reg, X, y)
plt.show()
```


    
![在这里插入图片描述](https://img-blog.csdnimg.cn/cb0042d322584e3884fe2b50a487e696.png#pic_center)

    



```python
from sklearn.pipeline import Pipeline
polynomial_regression = Pipeline([
    ('poly_features', PolynomialFeatures(degree=10, include_bias=False)),
    ('lin_reg', LinearRegression())
])
```


```python
poly_features = PolynomialFeatures(degree=10, include_bias=False)
X_poly = poly_features.fit_transform(X)
lin_reg = LinearRegression()
# lin_reg.fit(X_poly,y)
plot_learning_curves(lin_reg, X,y)
plt.show()
```


    
![在这里插入图片描述](https://img-blog.csdnimg.cn/f0984da0efd649e5b7aee4ed481a4ab5.png#pic_center)

    



```python
from sklearn.linear_model import Ridge
ridge_reg = Ridge(alpha=0.1, solver='cholesky')
ridge_reg.fit(X,y)
ridge_reg.predict([[1.5]])
```




    array([[5.01749775]])




```python
sgd_reg = SGDRegressor(penalty='l2')
sgd_reg.fit(X,y.ravel())
sgd_reg.predict([[1.5]])
```




    array([5.00194116])




```python
from sklearn.linear_model import Lasso
lasso_reg = Lasso(alpha=0.1)
lasso_reg.fit(X,y)
lasso_reg.predict([[1.5]])
```




    array([4.97201777])




```python
sgd_reg=SGDRegressor(penalty='l1')
sgd_reg.fit(X,y.ravel())
sgd_reg.predict([[1.5]])
```




    array([5.00530233])




```python
from sklearn.linear_model import ElasticNet
elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)
elastic_net.fit(X,y)
elastic_net.predict([[1.5]])
```




    array([4.97514089])




```python
from sklearn.preprocessing import StandardScaler
```


```python
from sklearn.base import clone
X_train, X_val, y_train, y_val = train_test_split(X, y.ravel(), test_size=0.2)
poly_scaler = Pipeline([
    ('poly_features', PolynomialFeatures(degree=90, include_bias=False)),
    ('std_scaler', StandardScaler())
])
X_train_poly_scaled = poly_scaler.fit_transform(X_train)
X_val_poly_scaled = poly_scaler.transform(X_val)
sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True,
                      penalty=None, learning_rate='constant', eta0=0.0005)
minimum_val_error = float('inf')
best_epoch = None
best_model = None
for epoch in range(1000):
    sgd_reg.fit(X_train_poly_scaled, y_train)
    y_val_predict = sgd_reg.predict(X_val_poly_scaled)
    val_error = mean_squared_error(y_val, y_val_predict)
    if val_error < minimum_val_error:
        minimum_val_error = val_error
        best_epoch = epoch
        best_model = clone(sgd_reg)
```


```python
t = np.linspace(-10, 10, 100)
# 函数
sig = 1 / (1 + np.exp(-t))
# 图片大小
plt.figure(figsize=(9,3))
# y=0给条实线 y=0.5、y=1给条虚线 x=0实线 
plt.plot([-10,10],[0,0],'k-')
plt.plot([-10,10], [0.5,0.5], 'k:')
plt.plot([-10,10],[1,1], 'k:')
plt.plot([0,0], [-1.1,1.1], 'k-')
plt.plot(t, sig, 'b-', linewidth=2, label=r'$\sigma(t) = \frac{1}{1 + e^{-t}}$')
plt.xlabel('t')
plt.legend(loc='upper left', fontsize=20)
plt.axis([-10, 10, -0.1, 1.1])
# save_fig('logistic_function_plot')
plt.show()
```


    
![在这里插入图片描述](https://img-blog.csdnimg.cn/e7d757df4dc9450e8eeca1ba01fc2af8.png#pic_center)

    



```python
from sklearn import datasets
iris = datasets.load_iris()
list(iris.keys())
```




    ['data',
     'target',
     'frame',
     'target_names',
     'DESCR',
     'feature_names',
     'filename',
     'data_module']




```python
X = iris['data'][:, 3:]
y = (iris['target'] == 2).astype(int)
```


```python
from sklearn.linear_model import LogisticRegression
log_reg = LogisticRegression()
log_reg.fit(X,y)
```




    LogisticRegression()




```python
# 数据X：第四行后的花瓣宽度；y：target列为2（virginia）则True（1），否则0
# X轴
X_new = np.linspace(0,3,1000).reshape(-1,1)
# 相应逻辑回归的类别概率值
y_proba = log_reg.predict_proba(X_new)
# 决策边界：可能性刚好在1/2的x点
decision_boundary = X_new[y_proba[:, 1] >= 0.5][0]
plt.figure(figsize=(8,3))
# y包含50个1，100个0（非弗吉尼亚）
# X[y==0], y[y==0]返回100个非弗花瓣宽度、100个0，标记蓝色方块
# X[y==1], y[y==1]返回50个弗花瓣宽度、50个1，标记绿色正三角
plt.plot(X[y==0], y[y==0], 'bs')
plt.plot(X[y==1], y[y==1], 'g^')
# 决策边界虚线
plt.plot([decision_boundary, decision_boundary], [-1,2], 'k:', linewidth=2)
# （第二列）正类的相对概率、负类
plt.plot(X_new, y_proba[:, 1], 'g-', linewidth=2, label='弗吉尼亚鸢尾')
plt.plot(X_new, y_proba[:, 0], 'b--', linewidth=2, label='非弗吉尼亚鸢尾')
# 文本坐标、垂直对齐方式
plt.text(decision_boundary+0.02, 0.15, '决策边界', fontsize=14, color='k', ha='center')
# 箭尾坐标、箭头方向与坐标偏移量、头宽长、头尾色
plt.arrow(decision_boundary, 0.08, -0.3, 0, head_width=0.05, head_length=0.1, fc='b', ec='b')
plt.arrow(decision_boundary, 0.92, 0.3, 0, head_width=0.05, head_length=0.1, fc='g', ec='g')
plt.xlabel('花瓣宽度（cm）', fontsize=14)
plt.ylabel('概率', fontsize=14)
plt.legend(loc='center left', fontsize=14)
plt.axis([0,3,-0.02,1.02])
plt.show()
```

 



    
![在这里插入图片描述](https://img-blog.csdnimg.cn/f2ef5e5b406e442e9a0f6be572395681.png#pic_center)

    



```python
log_reg.predict([[1.7],[1.5]])
```




    array([1, 0])




```python
Xl,Yl = np.meshgrid(
        np.linspace(0,1000,20).reshape(-1,1), 
        np.linspace(0,500,20).reshape(-1,1)
)
plt.plot(Xl, Yl,
         color='limegreen',  # 设置颜色为limegreen
         marker='.',  # 设置点类型为圆点
         linestyle='')  # 设置线型为空，也即没有线连接点
plt.grid(True)
plt.show()
```


    
![在这里插入图片描述](https://img-blog.csdnimg.cn/96371ad573324b0aa1b132f9a2041dfa.png#pic_center)

    



```python
from sklearn.linear_model import LogisticRegression
# 花瓣长度、宽度
X = iris['data'][:, (2,3)]
# 非0弗1，前100后50
y = (iris['target'] == 2).astype(int)
# 五种优化之一、系数倒数
log_reg = LogisticRegression(solver='lbfgs', C=10**10, random_state=42)
log_reg.fit(X, y)
# 生成网格点矩阵，均200行500列
x0, x1 = np.meshgrid(
        np.linspace(2.9,7,500).reshape(-1,1),
        np.linspace(0.8,2.7,200).reshape(-1,1)
)
# 分别拉成一维，再合并
X_new = np.c_[x0.ravel(), x1.ravel()]
# 对X_new各点给出0/1分数
y_proba = log_reg.predict_proba(X_new)
plt.figure(figsize=(10,4))
# 列表前100T后50F，加第二位的0/1。。。
# 100个非弗行，以及第一列的长度第二列的宽度
plt.plot(X[y==0, 0], X[y==0,1], 'bs')
# 50个弗行，以及第一列的长度第二列的宽度
plt.plot(X[y==1, 0], X[y==1,1], 'g^')
zz = y_proba[:,1].reshape(x0.shape)
# 双自变，1因变，非填充的渐变..轮廓线，类似等高
contour = plt.contour(x0, x1, zz, cmap=plt.cm.brg)
left_right = np.array([2.9, 7])
boundary = -(log_reg.coef_[0][0] * left_right + log_reg.intercept_[0]) / log_reg.coef_[0][1]
# 
plt.clabel(contour, inline=1, fontsize=12)
plt.plot(left_right, boundary, 'k--', linewidth=3)
plt.text(3.5, 1.5, '非弗', fontsize=14, color='b', ha='center')
plt.text(6.5, 2.3, '弗', fontsize=14, color='g', ha='center')
plt.xlabel('花瓣长度', fontsize=14)
plt.ylabel('花瓣宽度', fontsize=14)
plt.axis([2.9,7,0.8,2.7])
plt.show()
```


    
![在这里插入图片描述](https://img-blog.csdnimg.cn/27a95677273c4be5bf1210a73b828eec.png#pic_center)

    



```python
X = iris['data'][:, (2,3)]
y = iris['target']
softmax_reg = LogisticRegression(multi_class='multinomial',solver='lbfgs', C=10)
softmax_reg.fit(X,y)
softmax_reg.predict([[5,2]])
```




    array([2])




```python
softmax_reg.predict_proba([[5,2]])
```




    array([[6.38014896e-07, 5.74929995e-02, 9.42506362e-01]])




```python
# 用softmax批量梯度下降，实现提前停止
X = iris['data'][:, (2,3)]
y = iris['target']
# 构造矩阵，对每个矩阵添加x0为1的偏置项
X_with_bias = np.c_[np.ones([len(X), 1]), X]
np.random.seed(2042)
# 手动实现train与val集分层抽样、设定test、val比例与总数
test_ratio = 0.2
validation_ratio = 0.2
total_size = len(X_with_bias)
# 设定各数据集的数量
test_size = int(total_size * test_ratio)
validation_size = int(total_size * validation_ratio)
train_size = total_size - test_size - validation_size
# permutation:对总量索引随机排序，二维0纵1横，三维0横向1纵向，
rnd_indices = np.random.permutation(total_size)
# 各数据随机切片操作
X_train = X_with_bias[rnd_indices[:train_size]]
y_train = y[rnd_indices[:train_size]]
X_valid = X_with_bias[rnd_indices[train_size:-test_size]]
y_valid = y[rnd_indices[train_size:-test_size]]
X_test = X_with_bias[rnd_indices[-test_size:]]
y_test = y[rnd_indices[-test_size:]]
# 编稀疏模型（独热编码），模型全0之后，只要对应索引为1
def to_one_hot(y):
    n_classes = y.max() + 1
    m = len(y)
    Y_one_hot = np.zeros((m, n_classes))
    Y_one_hot[np.arange(m), y] = 1
    return Y_one_hot

```


```python
y_train[:10]
```




    array([0, 1, 2, 1, 1, 0, 1, 1, 1, 0])




```python
to_one_hot(y_train[:10])
```




    array([[1., 0., 0.],
           [0., 1., 0.],
           [0., 0., 1.],
           [0., 1., 0.],
           [0., 1., 0.],
           [1., 0., 0.],
           [0., 1., 0.],
           [0., 1., 0.],
           [0., 1., 0.],
           [1., 0., 0.]])




```python
# 目标标签全转稀疏独热模型
Y_train_one_hot = to_one_hot(y_train)
Y_valid_one_hot = to_one_hot(y_valid)
Y_test_one_hot = to_one_hot(y_test)
```


```python
# 指数除以指数之和函数
def softmax(logits):
    exps = np.exp(logits)
    exp_sums = np.sum(exps, axis=1, keepdims=True)
    return exps / exp_sums
```


```python
# 特征数量、标签类别数量
n_inputs = X_train.shape[1]
n_outputs = len(np.unique(y_train))
```


```python
# 训练模型需要的等式
# 成本函数
# $J(\mathbf{\Theta}) =
# - \dfrac{1}{m}\sum\limits_{i=1}^{m}\sum\limits_{k=1}^{K}{y_k^{(i)}\log\left(\hat{p}_k^{(i)}\right)}$
# 梯度计算
# $\nabla_{\mathbf{\theta}^{(k)}} \, J(\mathbf{\Theta}) = 
# \dfrac{1}{m} \sum\limits_{i=1}^{m}{ \left ( \hat{p}^{(i)}_k - y_k^{(i)} \right ) \mathbf{x}^{(i)}}$
eta = 0.01
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7
Theta = np.random.randn(n_inputs, n_outputs)
# softmax实现批量梯度下降
for iteration in range(n_iterations):
    logits = X_train.dot(Theta)
    Y_proba = softmax(logits)
    if iteration % 500 == 0:
        loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba+epsilon), axis=1))
        print(iteration, loss)
    error = Y_proba - Y_train_one_hot
    gradients = 1/m * X_train.T.dot(error)
    Theta = Theta - eta * gradients
```

    0 5.446205811872683
    500 0.8350062641405651
    1000 0.6878801447192402
    1500 0.6012379137693313
    2000 0.5444496861981872
    2500 0.5038530181431525
    3000 0.47292289721922487
    3500 0.44824244188957774
    4000 0.4278651093928793
    4500 0.41060071429187134
    5000 0.3956780375390374



```python
Theta
```




    array([[ 3.32094157, -0.6501102 , -2.99979416],
           [-1.1718465 ,  0.11706172,  0.10507543],
           [-0.70224261, -0.09527802,  1.4786383 ]])




```python
# 预测验证集、检查概率分数
logits = X_valid.dot(Theta)
Y_proba = softmax(logits)
y_predict = np.argmax(Y_proba, axis=1)

accuracy_score = np.mean(y_predict == y_valid)
accuracy_score
```




    0.9666666666666667




```python
# 对成本函数添加l2惩罚
eta = 0.1
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7
alpha = 0.1
Theta = np.random.randn(n_inputs, n_outputs)
for iteration in range(n_iterations):
    logits = X_train.dot(Theta)
    Y_proba = softmax(logits)
    if iteration % 500 == 0:
        xentropy_loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))
        l2_loss = 1/2 * np.sum(np.square(Theta[1:]))
        loss = xentropy_loss + alpha * l2_loss
        print(iteration, loss)
    error = Y_proba - Y_train_one_hot
    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]
    Theta = Theta - eta * gradients
```

    0 5.401014020496038
    500 0.5399802167300589
    1000 0.5055073771883054
    1500 0.4953639890209271
    2000 0.49156703270914
    2500 0.4900134074001495
    3000 0.48934877664358845
    3500 0.48905717267345383
    4000 0.488927251858594
    4500 0.4888688023117297
    5000 0.4888423408562912



```python
# 验证模型表现
logits = X_valid.dot(Theta)
Y_proba = softmax(logits)
y_predict = np.argmax(Y_proba, axis=1)

accuracy_score = np.mean(y_predict == y_valid)
accuracy_score
```




    1.0




```python
#  提前停止算法：在每次迭代计算Eval，并在当Eval开始上升时停止
eta = 0.1
n_iterations = 5001
m =len(X_train)
epsilon = 1e-7
alpha = 0.1
best_loss = np.infty
Theta = np.random.randn(n_inputs, n_outputs)
for iteration in range(n_iterations):
    logits = X_train.dot(Theta)
    Y_proba = softmax(logits)
    error = Y_proba - Y_train_one_hot
    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]
    Theta = Theta -eta * gradients
    
    logits = X_valid.dot(Theta)
    Y_proba = softmax(logits)
    xentropy_loss = -np.mean(np.sum(Y_valid_one_hot * np.log(Y_proba + epsilon), axis=1))
    l2_loss = 1/2 * np.sum(np.square(Theta[1:]))
    loss = xentropy_loss + alpha * l2_loss
    if iteration % 500 == 0:
        print(iteration, loss)
    if loss < best_loss:
        best_loss = loss
    else:
        print(iteration -1, best_loss)
        print(iteration, loss, '提前停止！')
        break
```

    0 2.897275838876366
    500 0.5702751662442892
    1000 0.5425654873413586
    1500 0.5353090385301479
    2000 0.5331256731252507
    2500 0.5325827330917428
    2736 0.5325454243382794
    2737 0.5325454252101579 提前停止！



```python
logits = X_valid.dot(Theta)
Y_proba = softmax(logits)
y_predict = np.argmax(Y_proba, axis=1)
accuracy_score = np.mean(y_predict == y_valid)
accuracy_score
```




    1.0




```python
# 绘制模型预测图
x0, x1 = np.meshgrid(
        np.linspace(0, 8, 500).reshape(-1,1),
        np.linspace(0, 3.5, 200).reshape(-1,1)
)
X_new = np.c_[x0.ravel(), x1.ravel()]
X_new_with_bias = np.c_[np.ones([len(X_new), 1]), X_new]
logits = X_new_with_bias.dot(Theta)
Y_proba = softmax(logits)
y_predict = np.argmax(Y_proba, axis=1)
zz1 = Y_proba[:, 1].reshape(x0.shape)
zz = y_predict.reshape(x0.shape)
plt.figure(figsize=(10, 4))
plt.plot(X[y==2,0], X[y==2,1], 'g^', label='弗吉尼亚')
plt.plot(X[y==1,0], X[y==1,1], 'bs', label='变色鸢尾')
plt.plot(X[y==0,0], X[y==0,1], 'yo', label='山鸢尾')
from matplotlib.colors import ListedColormap
custom_cmap = ListedColormap(['#fafab0', '#9898ff', '#a0faa0'])
plt.contourf(x0, x1, zz, cmap=custom_cmap)
contour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)
plt.clabel(contour, inline=1, fontsize=12)
plt.xlabel('花瓣长度',fontsize=14)
plt.ylabel('花瓣宽度',fontsize=14)
plt.legend(loc='upper left', fontsize=14)
plt.axis([0,7,0,3.5])
plt.show()
```


    
![在这里插入图片描述](https://img-blog.csdnimg.cn/2c6b1d28c0d94369abaa72badcfc50fc.png#pic_center)

    



```python
# 对测试集测试模型准确度
# 原因：N太小，并且对三数据集切分不同结果也不同
logits = X_test.dot(Theta)
Y_proba = softmax(logits)
y_predict = np.argmax(Y_proba, axis=1)
accuracy_score = np.mean(y_predict == y_test)
accuracy_score
```




    0.9333333333333333
