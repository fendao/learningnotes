```python
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import matplotlib as mpl
import seaborn as  sns
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
plt.rcParams.update({'figure.max_open_warning':0})
import warnings
warnings.filterwarnings('ignore')
from scipy import stats
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV, RepeatedKFold, cross_val_score
from sklearn.model_selection import cross_val_predict, KFold
from sklearn.metrics import make_scorer, mean_squared_error
from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet
from sklearn.svm import LinearSVR, SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from xgboost import XGBRegressor
from sklearn.preprocessing import PolynomialFeatures, MinMaxScaler, StandardScaler
```


```python
from jupyterthemes import jtplot
jtplot.style('monokai')
```

# 导入

## 导入数据


```python
train_data_file = './data/zhengqi_train.txt'
test_data_file = './data/zhengqi_test.txt'

data_train = pd.read_csv(train_data_file, sep='\t', encoding='utf-8')
data_test = pd.read_csv(test_data_file, sep='\t', encoding='utf-8')
```

## 合并数据


```python
# 注意各自增加了originin特征与值
data_train['oringin'] = 'train'
data_test['oringin'] = 'test'
data_all = pd.concat([data_train, data_test], axis=0, ignore_index=True)
```

## 删除特征


```python
data_all.drop(['V5', 'V9', 'V11', 'V17', 'V22', 'V28'], axis=1, inplace=True)
```

## 归一化


```python
cols_numeric = list(data_all.columns)
cols_numeric.remove('oringin')
def scale_minmax(col):
    return (col-col.min()) / (col.max()-col.min())
scale_cols = [col for col in cols_numeric if col != 'target']
data_all[scale_cols] = data_all[scale_cols].apply(scale_minmax, axis=0)
```

## 六个图看关系


```python
fcols = 6
frows = len(cols_numeric)
plt.figure(figsize=(4 * fcols, 4 * frows))
i = 0
# 前13个列索引/特征，V0、1、2、3、4、6、7、8、10、12、13、14、15
for var in cols_numeric:
    if var !='target':
        # 13个列分别与target列
        dat = data_all[[var, 'target']].dropna()
        # 第一图绘制各特征变量的数据频率是否拟合标正
        i += 1
        plt.subplot(frows, fcols, i)
        sns.distplot(dat[var], fit=stats.norm)
        plt.title(var + ' Original')
        plt.xlabel('')
        plt.ylabel('')
        # 第二图绘制各特征变量数据分布是否拟合标正，计算特征变量数据偏度
        i += 1
        plt.subplot(frows, fcols, i)
        _ = stats.probplot(dat[var], plot=plt)
        plt.title('skew=' + '{:.4f}'.format(stats.skew(dat[var])))
        plt.xlabel('')
        plt.ylabel('')
        # 第三图绘制各特征变量与标签变量散点数据，计算两变量的相关系数（第一行二列，二行一列均可。因是两者间系数矩阵
        i += 1
        plt.subplot(frows, fcols, i)
        plt.plot(dat[var], dat['target'], '.', alpha=0.5)
        plt.title('corr=' + '{:.2f}'.format(np.corrcoef(dat[var], dat['target'])[0][1]))
        # 第四图数据正态化、且归一化，绘制正态归一化后的数据，直方图是否拟合标正
        i += 1
        plt.subplot(frows, fcols, i)
        trans_var, lambda_var = stats.boxcox(dat[var].dropna() + 1)
        trans_var = scale_minmax(trans_var)
        sns.distplot(trans_var, fit=stats.norm)
        plt.title(var + ' Transformed')
        plt.xlabel('')
        # 第五图绘制QQ图是否拟合标正，计算转换后的新偏度
        i += 1
        plt.subplot(frows, fcols, i)
        _ = stats.probplot(trans_var, plot=plt)
        plt.title('skew=' + '{:.4f}'.format(stats.skew(trans_var)))
        plt.xlabel('')
        plt.ylabel('')
        # 第六图绘制转换后特征变量与标签变量数据散点图，计算新特征变量与标签变量相关系数
        i += 1
        plt.subplot(frows, fcols, i)
        plt.plot(trans_var, dat['target'], '.', alpha=0.5)
        plt.title('corr=' + '{:.2f}'.format(np.corrcoef(trans_var, dat['target'])[0][1]))
```


    
![png](https://github.com/fendao/imgs/blob/main/tianchi_1_7/output_12_0.png)
    


## BoxCox变换


```python
cols_transform = data_all.columns[0:-2]
for col in cols_transform:
    data_all.loc[:, col], _ = stats.boxcox(data_all.loc[:, col]+1)
```

## 标签数据对数变换，分位数计算


```python
print(data_all.target.describe())

plt.figure(figsize=(12, 4))
plt.subplot(121)
sns.distplot(data_all.target.dropna(), fit=stats.norm)
plt.subplot(122)
_ = stats.probplot(data_all.target.dropna(), plot=plt)
```

    count    2888.000000
    mean        0.126353
    std         0.983966
    min        -3.044000
    25%        -0.350250
    50%         0.313000
    75%         0.793250
    max         2.538000
    Name: target, dtype: float64



    
![png](https://github.com/fendao/imgs/blob/main/tianchi_1_7/output_16_1.png)
    



```python
sp = data_train.target
data_train.target1 = np.power(1.5, sp)
print(data_train.target.describe())

plt.figure(figsize=(12, 4))
plt.subplot(121)
sns.distplot(data_train.target1.dropna(), fit=stats.norm)
plt.subplot(122)
_ = stats.probplot(data_train.target1.dropna(), plot=plt)
```

    count    2888.000000
    mean        0.126353
    std         0.983966
    min        -3.044000
    25%        -0.350250
    50%         0.313000
    75%         0.793250
    max         2.538000
    Name: target, dtype: float64



    
![png](https://github.com/fendao/imgs/blob/main/tianchi_1_7/output_17_1.png)
    


# 获取训练数据并测试


```python
def get_training_data():
    df_train = data_all[data_all['oringin'] == 'train']
    df_train['label'] = data_train.target1
    
    y = df_train.target
    X = df_train.drop(['oringin', 'target', 'label'], axis=1)
    X_train, X_valid, y_train, y_valid = train_test_split(X,
                                                         y,
                                                         test_size=0.3,
                                                         random_state=13)
    return X_train, X_valid, y_train, y_valid
def get_test_data():
    df_test = data_all[data_all['oringin'] == 'test'].reset_index(drop=True)
    return df_test.drop(['oringin', 'target'], axis=1)
```

## 性能评价


```python
def rmse(y_true, y_pred):
    diff = y_pred - y_true
    sum_sq = sum(diff**2)
    n = len(y_pred)
    
    return np.sqrt(sum_sq/n)
def mse(y_true, y_pred):
    return mean_squared_error(y_true, y_pred)

rmse_scorer = make_scorer(rmse, greater_is_better=False)
mse_scorer = make_scorer(mse, greater_is_better=False)
```

## 获取异常数据并画图


```python
# 获取异常数据
def find_outliers(model, X, y, sigma=3):
    # 使用model预测y值
    try:
        y_pred = pd.Series(model.predict(X), index=y.index)
    # 预测失败就训练model
    except:
        model.fit(X, y)
        y_pred = pd.Series(model.predict(X), index=y.index)
    # 计算预测与真值之间的残差
    resid = y - y_pred
    mean_resid = resid.mean()
    std_resid = resid.std()
    # 计算统计值z，由z找到是否超出阙值
    z = (resid - mean_resid) / std_resid
    outliers = z[abs(z) > sigma].index
    
    # 输出结果
    print('R2系数=', model.score(X, y))
    print('均方根误差=', rmse(y, y_pred))
    print('均方误差=', mean_squared_error(y, y_pred))
    print('--------------------------------------')
    
    print('残差均值:', mean_resid)
    print('残差标准差:', std_resid)
    print('--------------------------------------')
    
    print(len(outliers), '个','离群值为:')
    print(outliers.tolist())
    
    plt.figure(figsize=(15, 5))
    ax_131 = plt.subplot(1, 3, 1)
    plt.plot(y, y_pred, '.')
    plt.plot(y.loc[outliers], y_pred.loc[outliers], 'ro')
    plt.legend(['Accepted', 'Outlier'])
    plt.xlabel('y')
    plt.ylabel('y_pred')
    
    ax_132 = plt.subplot(1, 3, 2)
    plt.plot(y, y-y_pred, '.')
    plt.plot(y.loc[outliers], y.loc[outliers]-y_pred[outliers], 'ro')
    plt.legend(['Accepted', 'Outlier'])
    plt.xlabel('y')
    plt.ylabel('y \n- \ny_pred', rotation=0, labelpad=6)
    
    ax_133 = plt.subplot(1, 3, 3)
    z.plot.hist(bins=50, ax=ax_133)
    z.loc[outliers].plot.hist(color='r', bins=50, ax=ax_133)
    plt.legend(['Accepted', 'Outlier'])
    plt.xlabel('z')
    
    plt.savefig('outliers.png')
    return outliers
```

## 保留与删除异常数据


```python
X_train, X_valid, y_train, y_valid = get_training_data()
test = get_test_data()

outliers = find_outliers(Ridge(), X_train, y_train)

X_outliers = X_train.loc[outliers]
y_outliers = y_train.loc[outliers]
X_t = X_train.drop(outliers)
y_t = y_train.drop(outliers)
```

    R2系数= 0.865606924643401
    均方根误差= 0.35532225979689636
    均方误差= 0.126253908307173
    --------------------------------------
    残差均值: 2.432492604176246e-16
    残差标准差: 0.3554101999689634
    --------------------------------------
    20 个 离群值为:
    [2647, 2842, 1310, 2769, 2160, 2668, 2667, 1523, 777, 1704, 2159, 1145, 348, 2620, 2807, 1874, 2696, 884, 2645, 344]



    
![png](https://github.com/fendao/imgs/blob/main/tianchi_1_7/output_25_1.png)
    



```python
def get_training_data_omitoutliers():
    y1 = y_t.copy()
    X1 = X_t.copy()
    return X1, y1
```

## 获取干净数据，网格搜索模型
打印最佳模型的分数、指标、验证均值标准差，
绘制真实值与预测
真值与残差
标准化的残差


```python
def train_model(model, param_grid=[], X=[], y=[], splits=5, repeats=5):
    if len(y) == 0:
        X, y = get_training_data_omitoutliers()
    # 交叉验证实例
    rkfold = RepeatedKFold(n_splits=splits, n_repeats=repeats)
    # 给参数格子就网格搜索实例
    if len(param_grid) > 0:
        gsearch = GridSearchCV(model,
                              param_grid,
                              cv=rkfold,
                              scoring='neg_mean_squared_error',
                              verbose=1,
                              return_train_score=True)
        # 网格搜索
        gsearch.fit(X, y)
        # 最好的模型
        model = gsearch.best_estimator_
        best_idx = gsearch.best_index_
        # 最好模型的验证分数
        grid_results = pd.DataFrame(gsearch.cv_results_)
        cv_mean = abs(grid_results.loc[best_idx, 'mean_test_score'])
        cv_std = grid_results.loc[best_idx, 'std_test_score']
    else:
        grid_results = []
        cv_results = cross_val_score(model,
                                    X,y,
                                    scoring='neg_mean_squared_error',
                                    cv=rkfold)
        cv_mean = abs(np.mean(cv_results))
        cv_std = np.std(cv_results)
    # 组合验证均值与标准差成为序列
    cv_score = pd.Series({'mean':cv_mean, 'std':cv_std})
    
    # 预测数据
    y_pred = model.predict(X)
    # 打印统计数据
    print('-----------------------')
    print(model)
    print('-----------------------')
    print('score=', model.score(X,y))
    print('rmse=', rmse(y, y_pred))
    print('mse=', mse(y, y_pred))
    print('cross_val: mean=', cv_mean, ', std=', cv_std)
    
    # 绘制标准化残差z
    y_pred = pd.Series(y_pred, index=y.index)
    resid = y - y_pred
    mean_resid = resid.mean()
    std_resid = resid.std()
    z = (resid - mean_resid) / std_resid
    n_outliers = sum(abs(z) > 3)
    
    plt.figure(figsize=(15, 5))
    ax_131 = plt.subplot(131)
    plt.plot(y, y_pred, '.')
    plt.xlabel('y')
    plt.ylabel('y_\npred', rotation=0, labelpad=1)
    plt.title('corr={:.3f}'.format(np.corrcoef(y, y_pred)[0][1]))
    ax_132 = plt.subplot(132)
    plt.plot(y, y-y_pred, '.')
    plt.xlabel('y')
    plt.ylabel('y\n-\ny_\npred', rotation=0, labelpad=1)
    plt.title('std resid = {:.3f}'.format(std_resid))
    
    ax_133 = plt.subplot(133)
    z.plot.hist(bins=50, ax=ax_133)
    plt.xlabel('z')
    plt.title('{:.0f} samples with z>3'.format(n_outliers))
    
    return model, cv_score, grid_results
# 放置最佳模型与验证分数
opt_models = dict()
score_models = pd.DataFrame(columns=['mean', 'std'])
splits=5
repeats=5
```

# 各模型预测

## 岭回归


```python
model = 'Ridge'
opt_models[model] = Ridge()
alph_range = np.arange(0.25, 6, 0.25)
param_grid = {'alpha': alph_range}

opt_models[model], cv_score, grid_results = train_model(opt_models[model],
                                                       param_grid=param_grid,
                                                       splits=splits,
                                                       repeats=repeats)
cv_score.name = model
score_models = score_models.append(cv_score)

plt.figure()
plt.errorbar(alph_range, abs(grid_results['mean_test_score']),
            abs(grid_results['std_test_score']) / np.sqrt(splits * repeats))
plt.xlabel('alpha')
plt.ylabel('s\nc\no\nr\ne', rotation=0, labelpad=10, linespacing=1)
plt.show()
```

    Fitting 25 folds for each of 23 candidates, totalling 575 fits
    -----------------------
    Ridge(alpha=0.25)
    -----------------------
    score= 0.8843733939112598
    rmse= 0.3264659531101768
    mse= 0.10658001854013605
    cross_val: mean= 0.11086661934122817 , std= 0.008530029760224447



    
![png](https://github.com/fendao/imgs/blob/main/tianchi_1_7/output_31_1.png)
    



    
![png](https://github.com/fendao/imgs/blob/main/tianchi_1_7/output_31_2.png)
    


## Lasso


```python
model = 'Lasso'
opt_models[model] = Lasso()

alph_range = np.arange(1e-4, 1e-3, 4e-5)
param_grid = {'alpha': alph_range}
opt_models[model], cv_score, grid_results = train_model(opt_models[model],
                                                       param_grid=param_grid,
                                                       splits=splits,
                                                       repeats=repeats)
cv_score.name = model
score_models = score_models.append(cv_score)

plt.figure()
plt.errorbar(alph_range, abs(grid_results['mean_test_score']),
            abs(grid_results['std_test_score']) / np.sqrt(splits * repeats))
plt.xlabel('alpha')
plt.ylabel('score')
plt.show()
```

    Fitting 25 folds for each of 23 candidates, totalling 575 fits
    -----------------------
    Lasso(alpha=0.0001)
    -----------------------
    score= 0.8844249768800051
    rmse= 0.3263931240174389
    mse= 0.10653247140586315
    cross_val: mean= 0.11149238270636125 , std= 0.006804427180010623



    
![png](https://github.com/fendao/imgs/blob/main/tianchi_1_7/output_33_1.png)
    



    
![png](https://github.com/fendao/imgs/blob/main/tianchi_1_7/output_33_2.png)
    


## 弹性网络


```python
model = 'ElasticNet'
opt_models[model] = ElasticNet()

param_grid = {
    'alpha': np.arange(1e-4, 1e-3, 1e-4),
    'l1_ratio': np.arange(0.1, 1.0, 0.1),
    'max_iter': [100000]
}
opt_models[model], cv_score, grid_results = train_model(opt_models[model],
                                                       param_grid=param_grid,
                                                       splits=splits,
                                                       repeats=1)
cv_score.name = model
scores_models = score_models.append(cv_score)
```

    Fitting 5 folds for each of 81 candidates, totalling 405 fits
    -----------------------
    ElasticNet(alpha=0.0001, l1_ratio=0.1, max_iter=100000)
    -----------------------
    score= 0.8844230834225808
    rmse= 0.32639579764421367
    mse= 0.10653421671980264
    cross_val: mean= 0.1128944745053646 , std= 0.011436351963507048



    
![png](https://github.com/fendao/imgs/blob/main/tianchi_1_7/output_35_1.png)
    


## SVR


```python
model = 'LinearSVR'
opt_models[model] = LinearSVR()

crange = np.arange(0.1, 1.0, 0.1)
param_grid = {'C': crange, 'max_iter': [1000]}
opt_models[model], cv_score, grid_results = train_model(opt_models[model],
                                                       param_grid=param_grid,
                                                       splits=splits,
                                                       repeats=repeats)
cv_score.name = model
score_models = score_models.append(cv_score)

plt.figure()
plt.errorbar(crange,
            abs(grid_results['mean_test_score']),
            abs(grid_results['std_test_score']) / np.sqrt(splits * repeats))
plt.xlabel('C')
plt.ylabel('score')
plt.show()
```

    Fitting 25 folds for each of 9 candidates, totalling 225 fits
    -----------------------
    LinearSVR(C=0.9)
    -----------------------
    score= 0.2949815753095215
    rmse= 0.8061377149869544
    mse= 0.6498580155243872
    cross_val: mean= 1.1071670767925488 , std= 0.8994013045819577



    
![png](https://github.com/fendao/imgs/blob/main/tianchi_1_7/output_37_1.png)
    



    
![png](https://github.com/fendao/imgs/blob/main/tianchi_1_7/output_37_2.png)
    


## K近邻


```python
model = 'KNeighbors'
opt_models[model] = KNeighborsRegressor()
param_grid = {'n_neighbors': np.arange(3, 11, 1)}
opt_models[model], cv_score, grid_results = train_model(opt_models[model],
                                                       param_grid=param_grid,
                                                       splits=splits,
                                                       repeats=1)
cv_score.name = model
score_models = scores_models.append(cv_score)

plt.figure()
plt.errorbar(np.arange(3, 11 ,1),
            abs(grid_results['mean_test_score']),
            abs(grid_results['std_test_score']) / np.sqrt(splits * 1))
plt.xlabel('n_neighbors')
plt.ylabel('score')
plt.show()
```

    Fitting 5 folds for each of 8 candidates, totalling 40 fits
    -----------------------
    KNeighborsRegressor(n_neighbors=10)
    -----------------------
    score= 0.7102797151401347
    rmse= 0.5167713858829323
    mse= 0.26705266526736626
    cross_val: mean= 0.3318952162340149 , std= 0.018260504852919187



    
![png](https://github.com/fendao/imgs/blob/main/tianchi_1_7/output_39_1.png)
    



    
![png](https://github.com/fendao/imgs/blob/main/tianchi_1_7/output_39_2.png)
    


# Boosting模型融合

## GBDT


```python
model = 'GradientBoosting'
opt_models[model] = GradientBoostingRegressor()

param_grid = {
    'n_estimators':[150,250,350],
    'max_depth':[1,2,3],
    'min_samples_split':[5,6,7]
}
opt_models[model], cv_score, grid_results = train_model(opt_models[model],
                                                       param_grid=param_grid,
                                                       splits=splits,
                                                       repeats=1)
cv_score.name = model
score_models = score_models.append(cv_score)
```

    Fitting 5 folds for each of 27 candidates, totalling 135 fits
    -----------------------
    GradientBoostingRegressor(max_depth=2, min_samples_split=5, n_estimators=350)
    -----------------------
    score= 0.9453841999011577
    rmse= 0.22437174042438324
    mse= 0.050342677901066724
    cross_val: mean= 0.10005230342732149 , std= 0.006900061918144573



    
![png](https://github.com/fendao/imgs/blob/main/tianchi_1_7/output_42_1.png)
    


## XGB


```python
model = 'XGB'
opt_models[model] = XGBRegressor(objective='reg:squarederror')
param_grid = {
    'n_estimators':[100,200,300,400,500],
    'max_depth':[1,2,3]
}
opt_models[model], cv_score, grid_results = train_model(opt_models[model],
                                                       param_grid=param_grid,
                                                       splits=splits,
                                                       repeats=1)
cv_score.name = model
score_models = score_models.append(cv_score)
```

    Fitting 5 folds for each of 15 candidates, totalling 75 fits
    -----------------------
    XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
                 colsample_bynode=1, colsample_bytree=1, enable_categorical=False,
                 gamma=0, gpu_id=-1, importance_type=None,
                 interaction_constraints='', learning_rate=0.300000012,
                 max_delta_step=0, max_depth=2, min_child_weight=1, missing=nan,
                 monotone_constraints='()', n_estimators=200, n_jobs=8,
                 num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,
                 reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact',
                 validate_parameters=1, verbosity=None)
    -----------------------
    score= 0.9580134672764903
    rmse= 0.1967270196044186
    mse= 0.03870152024243725
    cross_val: mean= 0.10520906279665454 , std= 0.009906195090672197



    
![png](https://github.com/fendao/imgs/blob/main/tianchi_1_7/output_44_1.png)
    


## 随机森林


```python
model = 'RandomForest'
opt_models[model] = RandomForestRegressor()
param_grid = {
    'n_estimators':[100,150,200],
    'max_features':[8,12,16,20,24],
    'min_samples_split':[2,4,6]
}
opt_models[model], cv_score, grid_results = train_model(opt_models[model],
                                                       param_grid=param_grid,
                                                       splits=5,
                                                       repeats=1)
cv_score.name = model
score_models = score_models.append(cv_score)
```

    Fitting 5 folds for each of 45 candidates, totalling 225 fits
    -----------------------
    RandomForestRegressor(max_features=16, n_estimators=200)
    -----------------------
    score= 0.9848036935285599
    rmse= 0.118352661406415
    mse= 0.014007352461981504
    cross_val: mean= 0.10544301232983937 , std= 0.008511170732127291



    
![png](https://github.com/fendao/imgs/blob/main/tianchi_1_7/output_46_1.png)
    


# Bagging


```python
def model_predict(test_data, test_y=[], stack=False):
    i=0
    y_predict_total = np.zeros((test_data.shape[0],))
    for model in opt_models.keys():
        # 两个否定的and，相当于两个肯定的or
        if model != 'LinearSVR' and model != 'KNeighbors':
            y_predict = opt_models[model].predict(test_data)
            # 各模型预测数据的累加
            y_predict_total += y_predict
            i+=1
        # 打印单模型的均方误差
        if len(test_y)>0:
            print('{}_mse'.format(model), mean_squared_error(y_predict, test_y))
    # 全部模型预测值的平均值 并3位取整
    y_predict_mean = np.round(y_predict_total/i, 3)
    if len(test_y) > 0:
        print('mean_mse:', mean_squared_error(y_predict_mean, test_y))
    else:
        y_predict_mean = pd.Series(y_predict_mean)
        return y_predict_mean
model_predict(X_valid, y_valid)
```

    Ridge_mse 0.11886121426170143
    Lasso_mse 0.11873332347343916
    ElasticNet_mse 0.11876396043941885
    LinearSVR_mse 0.11876396043941885
    KNeighbors_mse 0.11876396043941885
    GradientBoosting_mse 0.11970226830534904
    XGB_mse 0.12186328017876338
    RandomForest_mse 0.11696757810210494
    mean_mse: 0.10492423644752018


# Stacking


```python
from scipy import sparse
import xgboost
import lightgbm
```


```python
def stacking_reg(clf, train_x, train_y, test_x, clf_name, kf, label_split=None):
    # 放对子集2的预测数据，做第二层训练集
    train = np.zeros((train_x.shape[0], 1))
    test = np.zeros((test_x.shape[0], 1))
    # 放各预测器对测试数据预测
    test_pre = np.empty((folds, test_x.shape[0], 1))
    cv_scores = []
    for i, (train_index, test_index) in enumerate(kf.split(train_x, label_split)):
        # 训练集分成子集1、2
        tr_x = train_x[train_index]
        tr_y = train_y[train_index]
        te_x = train_x[test_index]
        te_y = train_y[test_index]
        if clf_name in ['rf', 'ada', 'gb', 'et', 'lr', 'lsvc', 'knn']:
            clf.fit(tr_x, tr_y)
            # 对测试数据子集2预测数据，做成第二层训练集
            pre = clf.predict(te_x).reshape(-1, 1)
            train[test_index] = pre
            # 预测测试数据，
            test_pre[i, :] = clf.predict(test_x).reshape(-1, 1)
            cv_scores.append(mean_squared_error(te_y, pre))
        elif clf_name in ['xgb']:
            train_matrix = clf.DMatrix(tr_x, label=tr_y, missing=-1)
            test_matrix = clf.DMatrix(te_x, label=te_y, missing=-1)
            z = clf.DMatrix(test_x, label=te_y, missing=-1)
            params = {
                'booster': 'gbtree',
                'eval_metrics': 'rmse',
                'gamma':1,
                'min_child_weight':1.5,
                'max_depth':5,
                'lambda':10,
                'subsample':0.7,
                'colsample_bytree':0.7,
                'colsample_bylevel':0.7,
                'eta':0.03,
                'tree_method':'exact',
                'seed':2017,
                'nthread':12
            }
            num_round = 10000
            early_stopping_rounds = 100
            watchlist = [(train_matrix, 'train'), (test_matrix, 'eval')]
            if test_matrix:
                model = clf.train(
                    params,
                    train_matrix,
                    num_boost_round=num_round,
                    evals=watchlist,
                    early_stopping_rounds=early_stopping_rounds)
                pre = model.predict(
                    test_matrix,
                    ntree_limit=model.best_ntree_limit).reshape(-1, 1)
                train[test_index] = pre
                test_pre[i, :] = model.predict(z,
                                              ntree_limit=model.best_ntree_limit).reshape(-1, 1)
                cv_scores.append(mean_squared_error(te_y, pre))
        elif clf_name in ['lgb']:
            train_matrix = clf.Dataset(tr_x, label=tr_y)
            test_matrix = clf.Dataset(te_x, label=te_y)
            params = {
                'boosting_tyte':'gbdt',
                'objective':'regression_l2',
                'metric':'mse',
                'min_child_weight':1.5,
                'num_leaves':2**5,
                'lambda_l2':10,
                'subsample':0.7,
                'colsample_bytree':0.7,
                'colsample_bylevel':0.7,
                'learning_rate':0.03,
                'tree_method':'exact',
                'seed':2022,
                'nthread':12,
                'silent':True
            }
            num_round = 10000
            early_stopping_rounds = 100
            if test_matrix:
                model = clf.train(
                    params,
                    train_matrix,
                    num_round,
                    valid_sets=test_matrix,
                    early_stopping_rounds=early_stopping_rounds)
                pre = model.predict(
                    te_x,
                    num_iteration=model.best_iteration).reshape(-1, 1)
                train[test_index] = pre
                test_pre[i, :] = model.predict(test_x,
                                              num_iteration=model.best_iteration).reshape(-1, 1)
                cv_scores.append(mean_squared_error(te_y, pre))
        else:
            raise IOError('Please add new clf.')
        print('%s now score is:' % clf_name, cv_scores)
    test[:] = test_pre.mean(axis=0)
    print('%s_score_list:' % clf_name, cv_scores)
    print('%s_score_mean:' % clf_name, np.mean(cv_scores))
    return train.reshape(-1, 1), test.reshape(-1, 1)
```

## stacking的基学习器


```python
def rf_reg(x_train, y_train, x_valid, kf, label_split=None):
    randomforest = RandomForestRegressor(n_estimators=600,
                                        max_depth=20,
                                        n_jobs=-1,
                                        random_state=2022,
                                        max_features='auto',
                                        verbose=1)
    rf_train, rf_test = stacking_reg(randomforest,
                                    x_train,
                                    y_train,
                                    x_valid,
                                    'rf',
                                    kf,
                                    label_split=label_split)
    return rf_train, rf_test, 'rf_reg'
def ada_reg(x_train, y_train, x_valid, kf, label_split=None):
    adaboost = AdaBoostRegressor(n_estimators=30,
                                random_state=2022,
                                learning_rate=0.01)
    ada_train, ada_test = stacking_reg(adaboost,
                                      x_train,
                                      y_train,
                                      x_valid,
                                      'ada',
                                      kf,
                                      label_split=label_split)
    return ada_train, ada_test, 'ada_reg'
def gb_reg(x_train, y_train, x_valid, kf, label_split=None):
    gbdt = GradientBoostingRegressor(learning_rate=0.04,
                                    n_estimators=100,
                                    subsample=0.8,
                                    random_state=2022,
                                    max_depth=5,
                                    verbose=1)
    gbdt_train, gbdt_test = stacking_reg(gbdt,
                                        x_train,
                                        y_train,
                                        x_valid,
                                        'gb',
                                        kf,
                                        label_split=label_split)
    return gbdt_train, gbdt_test, 'gb_reg'
def et_reg(x_train, y_train, x_valid, kf, label_split=None):
    extratree = ExtraTreesRegressor(n_estimators=600,
                                   max_depth=35,
                                   max_features='auto',
                                   n_jobs=-1,
                                   random_state=2022,
                                   verbose=1)
    et_train, et_test = stacking_reg(extratree,
                                    x_train,
                                    y_train,
                                    x_valid,
                                    'et',
                                    kf,
                                    label_split=label_split)
    return et_train, et_test, 'et_reg'
def lr_reg(x_train, y_train, x_valid, kf, label_split=None):
    lr_reg = LinearRegression(n_jobs=-1)
    lr_train, lr_test = stacking_reg(lr_reg,
                                    x_train,
                                    y_train,
                                    x_valid,
                                    'lr',
                                    kf,
                                    label_split=label_split)
    return lr_train, lr_test, 'lr_reg'
def xgb_reg(x_train, y_train, x_valid, kf, label_split=None):
    xgb_train, xgb_test = stacking_reg(xgboost,
                                      x_train,
                                      y_train,
                                      x_valid,
                                      'xgb',
                                      kf,
                                      label_split=label_split)
    return xgb_train, xgb_test, 'xgb_reg'
def lgb_reg(x_train, y_train, x_valid, kf, label_split=None):
    lgb_train, lgb_test = stacking_reg(lightgbm,
                                      x_train,
                                      y_train,
                                      x_valid,
                                      'lgb',
                                      kf,
                                      label_split=label_split)
    return lgb_train, lgb_test, 'lgb_reg'
```

## stacking预测


```python
def stacking_pred(x_train, y_train, x_valid, kf,
                 clf_list, label_split=None, clf_fin='lgb', if_concat_origin=True):
    for k, clf_list in enumerate(clf_list):
        clf_list = [clf_list]
        column_list = []
        train_data_list = []
        test_data_list = []
        for clf in clf_list:
            train_data, test_data, clf_name = clf(x_train,
                                                 y_train,
                                                 x_valid,
                                                 kf,
                                                 label_split=label_split)
            train_data_list.append(train_data)
            test_data_list.append(test_data)
            column_list.append('clf_%s' % (clf_name))
    train = np.concatenate(train_data_list, axis=1)
    test = np.concatenate(test_data_list, axis=1)
    
    if if_concat_origin:
        train = np.concatenate([x_train, train], axis=1)
        test = np.concatenate([x_valid, test], axis=1)
    print(x_train.shape, train.shape)
    print(clf_name)
    print(clf_name in ['lgb'])
    if clf_fin in ['rf', 'ada', 'gb', 'et', 'lr', 'lsvc', 'knn']:
        if clf_fin in ['rf']:
            clf = RandomForestRegressor(n_estimators=600,
                                       max_depth=20,
                                       n_jobs=-1,
                                       random_state=2022,
                                       max_features='auto',
                                       verbose=1)
        elif clf_fin in ['ada']:
            clf = AdaBoostRegressor(n_estimators=30,
                                   random_state=2022,
                                   learning_rate=0.01)
        elif clf_fin in ['gb']:
            clf = GradientBoostingRegressor(learning_rate=0.04,
                                           n_estimators=600,
                                           max_depth=35,
                                           max_features='auto',
                                           n_jobs=-1,
                                           random_state=2022,
                                           verbose=1)
        elif clf_fin in ['xgb']:
            ckf = xgboost
            train_matrix = clf.DMatrix(train,
                                      label=y_train, missing=-1)
            test_matrix = clf.DMatrix(train,
                                     label=y_train, missing=-1)
            params = {
                'booster':'gbtree',
                'eval_metric':'rmse',
                'gamma':1,
                'min_child_weight':1.5,
                'max_depth':5,
                'lambda':10,
                'subsample':0.7,
                'colsample_bytree':0.7,
                'colsample_bylevel':0.7,
                'eta':0.03,
                'tree_method':'exact',
                'seed':2022,
                'nthread':12
            }
            num_round = 10000
            early_stopping_rounds = 100
            watchlist = [(train_matrix, 'train'), (test_matrix, 'eval')]
            model = clf.train(params,
                             train_matrix,
                             nim_boost_round=num_round,
                             eval=watchlist,
                             early_stopping_rounds=early_stopping_rounds)
            pre = model.predict(test,
                               ntree_limit=model.best_ntree_limit).reshape(-1, 1)
            return pre
        elif clf_fin in ['lgb']:
            print(clf_name)
            clf = lightgbm
            train_matrix = clf.Dataset(train, label=y_train)
            test_matrix = clf.Dataset(train, label=y_train)
            params = {
                'boosting_type':'gbdt',
                'objective':'regression_l2',
                'metric':'mse',
                'min_child_weight':1.5,
                'num_leaves':2**5,
                'lambda_l2':10,
                'subsample':0.7,
                'colsample_bytree':0.7,
                'colsample_bylevel':0.7,
                'learning_rate':0.03,
                'tree_method':'exact',
                'seed':2022,
                'nthread':12,
                'silent':True
            }
            num_round = 10000
            early_stopping_rounds = 100
            model = clf.train(params,
                             train_matrix,
                             num_round,
                             valid_sets=test_matrix,
                             early_stopping_rounds=early_stopping_rounds)
            print('pred')
            pre = model.predict(test,
                               num_iteration=model.best_iteration).reshape(-1, 1)
            print(pre)
            return pre
```

# 模型验证


```python
with open('./data/zhengqi_train.txt') as fr:
    data_train = pd.read_table(fr, sep='\t')
with open('./data/zhengqi_test.txt') as fr_test:
    data_test = pd.read_table(fr_test, sep='\t')
from sklearn.model_selection import StratifiedKFold, KFold
folds = 5
seed = 1
kf = KFold(n_splits=5, shuffle=True, random_state=2022)
```

# 融合线归与lgb预测


```python
x_train = data_train[data_test.columns].values
x_valid = data_test[data_test.columns].values
y_train = data_train['target'].values
clf_list = [lr_reg, lgb_reg]

pred = stacking_pred(x_train, y_train, x_valid, kf, clf_list, label_split=None,
                    clf_fin='lgb', if_concat_origin=True)
```

    lr now score is: [0.1077187756404677]
    lr now score is: [0.1077187756404677, 0.10425851078300669]
    lr now score is: [0.1077187756404677, 0.10425851078300669, 0.13474098093127534]
    lr now score is: [0.1077187756404677, 0.10425851078300669, 0.13474098093127534, 0.10719216659798919]
    lr now score is: [0.1077187756404677, 0.10425851078300669, 0.13474098093127534, 0.10719216659798919, 0.10557682444044665]
    lr_score_list: [0.1077187756404677, 0.10425851078300669, 0.13474098093127534, 0.10719216659798919, 0.10557682444044665]
    lr_score_mean: 0.11189745167863711
    [LightGBM] [Warning] Unknown parameter: colsample_bylevel
    [LightGBM] [Warning] Unknown parameter: tree_method
    [LightGBM] [Warning] Unknown parameter: silent
    [LightGBM] [Warning] Unknown parameter: boosting_tyte
    [LightGBM] [Warning] Unknown parameter: colsample_bylevel
    [LightGBM] [Warning] Unknown parameter: tree_method
    [LightGBM] [Warning] Unknown parameter: silent
    [LightGBM] [Warning] Unknown parameter: boosting_tyte
    [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000899 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 8846
    [LightGBM] [Info] Number of data points in the train set: 2310, number of used features: 38
    [LightGBM] [Warning] Unknown parameter: colsample_bylevel
    [LightGBM] [Warning] Unknown parameter: tree_method
    [LightGBM] [Warning] Unknown parameter: silent
    [LightGBM] [Warning] Unknown parameter: boosting_tyte
    [LightGBM] [Info] Start training from score 0.123687
    [1]	valid_0's l2: 0.924679
    Training until validation scores don't improve for 100 rounds
    ......
    Early stopping, best iteration is:
    [1779]	valid_0's l2: 0.106077
    lgb now score is: [0.089321256095143, 0.09671491700734577, 0.12751733766539203, 0.10607651232533863]
    [LightGBM] [Warning] Unknown parameter: colsample_bylevel
    [LightGBM] [Warning] Unknown parameter: tree_method
    [LightGBM] [Warning] Unknown parameter: silent
    [LightGBM] [Warning] Unknown parameter: boosting_tyte
    [LightGBM] [Warning] Unknown parameter: colsample_bylevel
    [LightGBM] [Warning] Unknown parameter: tree_method
    [LightGBM] [Warning] Unknown parameter: silent
    [LightGBM] [Warning] Unknown parameter: boosting_tyte
    [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000792 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 8845
    [LightGBM] [Info] Number of data points in the train set: 2311, number of used features: 38
    [LightGBM] [Warning] Unknown parameter: colsample_bylevel
    [LightGBM] [Warning] Unknown parameter: tree_method
    [LightGBM] [Warning] Unknown parameter: silent
    [LightGBM] [Warning] Unknown parameter: boosting_tyte
    [LightGBM] [Info] Start training from score 0.124900
    [1]	valid_0's l2: 0.952317
    Training until validation scores don't improve for 100 rounds
    ...... 
    Early stopping, best iteration is:
    [687]	valid_0's l2: 0.106625
    lgb now score is: [0.089321256095143, 0.09671491700734577, 0.12751733766539203, 0.10607651232533863, 0.10662544805577663]
    lgb_score_list: [0.089321256095143, 0.09671491700734577, 0.12751733766539203, 0.10607651232533863, 0.10662544805577663]
    lgb_score_mean: 0.10525109422979921
    (2888, 38) (2888, 39)
    lgb_reg
    False



```python

```
