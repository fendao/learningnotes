```python
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import matplotlib as mpl
import seaborn as  sns
from scipy import stats
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']

import warnings
warnings.filterwarnings('ignore')
```


```python
from jupyterthemes import jtplot
jtplot.style('monokai')
```


```python
np.random.seed(666)
x = np.random.uniform(-3.0, 3.0, size=100)
X = x.reshape(-1, 1)
y = 0.5 * x ** 2 + x + 2 + np.random.normal(0, 1, size=100)

plt.scatter(x, y)
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/tianchi_1_3/output_2_0.png)
    



```python
from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(X, y)
lin_reg.score(X, y)
```




    0.4953707811865009




```python
from sklearn.metrics import mean_squared_error
y_predict = lin_reg.predict(X)
mean_squared_error(y, y_predict)
```




    3.0750025765636577




```python
plt.scatter(x, y)
plt.plot(np.sort(x), y_predict[np.argsort(x)], 'r')
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/tianchi_1_3/output_5_0.png)
    



```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler

def PolynomialRegression(degree):
    return Pipeline([('poly', PolynomialFeatures(degree=degree)),
                    ('std_scaler', StandardScaler()),
                    ('lin_reg', LinearRegression())])
```


```python
poly2_reg = PolynomialRegression(2)
poly2_reg.fit(X, y)
y2_predict = poly2_reg.predict(X)
mean_squared_error(y, y2_predict)
```




    1.0987392142417858




```python
plt.scatter(x, y)
plt.plot(np.sort(x), y2_predict[np.argsort(x)], 'r')
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/tianchi_1_3/output_8_0.png)
    



```python
poly10_reg = PolynomialRegression(10)
poly10_reg.fit(X, y)
y10_predict = poly10_reg.predict(X)
display(mean_squared_error(y, y10_predict))

plt.scatter(x, y)
plt.plot(np.sort(x), y10_predict[np.argsort(x)], 'r')
plt.show()
```


    1.0508466763764153



    
![png](https://github.com/fendao/imgs/blob/main/tianchi_1_3/output_9_1.png)
    



```python
poly100_reg = PolynomialRegression(100)
poly100_reg.fit(X, y)
y100_predict = poly100_reg.predict(X)
display(mean_squared_error(y, y100_predict))

plt.scatter(x, y)
plt.plot(np.sort(x), y100_predict[np.argsort(x)], 'r')
plt.show()
```


    0.6815738234547528



    
![png](https://github.com/fendao/imgs/blob/main/tianchi_1_3/output_10_1.png)
    


# 模型调参


```python
from sklearn.datasets import load_iris
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split

iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data,
                                                   iris.target,
                                                   random_state=0)
print('训练集大小：{}测试集大小：{}'.format(X_train.shape[0], X_test.shape[0]))

best_score = 0
for gamma in [0.001, 0.01, 0.1, 1, 10, 100]:
    for C in [0.001, 0.01, 0.1, 1, 10, 100]:
        svm = SVC(gamma=gamma, C=C)
        svm.fit(X_train, y_train)
        score = svm.score(X_test, y_test)
        if score > best_score:
            best_score = score
            best_parameters = {'gamma': gamma, 'C':C}
print('Best score:{:.2f}'.format(best_score))
print('Best Parameters:{}'.format(best_parameters))
```

    训练集大小：112测试集大小：38
    Best score:0.97
    Best Parameters:{'gamma': 0.001, 'C': 100}


# 软投票


```python
import matplotlib.gridspec as gridspec
import itertools
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier

from mlxtend.classifier import EnsembleVoteClassifier
from mlxtend.data import iris_data
from mlxtend.plotting import plot_decision_regions

clf1 = LogisticRegression(random_state=0,
                         solver='lbfgs',
                         multi_class='auto')
clf2= RandomForestClassifier(random_state=0, n_estimators=100)
clf3 = SVC(random_state=0, probability=True, gamma='auto')
# 软投票实例
eclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3],
                             weights=[2,1,1],
                             voting='soft')
# 加载数据
X, y = iris_data()
X = X[:, [0, 2]]

# 绘制决策边界
gs = gridspec.GridSpec(1, 4)
fig = plt.figure(figsize=(16, 4))

for clf, lab, grd in zip([clf1, clf2, clf3, eclf],
                        ['Logistic Regression', 'Random Forest',
                        'RBF kernel SVM', 'Ensemble'],
                        itertools.product([0,1], repeat=2)):
    clf.fit(X, y)
    ax = plt.subplot(gs[0, grd[0] * 2 + grd[1]])
    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)
    plt.title(lab)
plt.show()
```


    
![png](https://github.com/fendao/imgs/blob/main/tianchi_1_3/output_14_0.png)
    



```python
for i in itertools.product([0,1], repeat=2):
    print(i,i[0])
```

    (0, 0) 0
    (0, 1) 0
    (1, 0) 1
    (1, 1) 1



```python

```
