# 一、关于学习
1.ML：用电脑模拟  观察——学习——技巧
2.ML的三个关键点：目标pattern、程序definition、数据data
3.相关领域：食品推荐、穿搭推荐、预测能源消耗、准确交通、预测评分
4.发放信用卡认识符号：
    用户信息x、目标函数f:x——y、算法A从假说集H选个最好的h作为最有希望的假说g:x——y
# 二、最好与反推
1.学科关系
    DataMinig：g作数据中的有趣特性
    Artificial Intelligence：找g即是聪明行为
    Statistic：g即是推论结果
2.感知器perceptron
    直线分类标签+1-1，线性分类器    
    h(x) = sign(∑WiXi-threshold) = sign(WX)，W权数、x特征
3.PLA算法：找分隔线g，即完美权数W(f)
    基本规则：每轮会有更大的内积值（更近的距离）  W(f)Wt+1 > W(f)Wt
# 三、
1.批量监督二元分类
    Wt有错代表  y与sign(WX)异号
3.多元分类——无监硬币识别
    无监：无y，，，难点：衡量分群的好坏
    半监：只有部分y，，，方式：增强学习，通过award暗示y
4.批量学习
# 四、
1.推论未知 Eout ≈ Ein
    Hoeffding不等式：橘球占比 Ein 与 Eout间误差的概率上限，表明当N足够大，上限越小，从而 v ≈ µ
2.结论
    对大样本N，可大概推测P{ h(x) ≠ f(x) }
    只要Ein(h) ≈ 0， 则PAC： Eout(h) ≈ 0
3.糟糕的样本
    对由坏D生出的许多h，有连接上限(由Hoeffding推导)
    连接上限结论：当样本量N足够大，且假说数量M有限，无论算法咋选，都有Eout ≈ Ein
# 五、确定数量M
1.训练与测试
    对大M：P{bad}不会趋于0、但可降低Ein
    对小M：P{bad}趋0、不可降Ein
2.有效线量（成长函数 MH(N) < 2^N ）
    两相似假说h1与h2的Ein、Eout也相似，从而导致连接上限高估
3.分类相似假说h
4.计算成长函数mH(N)
    正射线、正区间、凸集
5.选mH(N)代替M
6.断点（成长函数第一个有希望的点）
# 六、
1.断点限制
    限制函数（对断点k的最大MH(N)）
    当N<k,B(N, k)=2^N    当N=k,B(N, k)=2^N-1
3.限制函数的上限
    B(N, k) ≤ B(N-1, k) + B(N-1, k-1)
4.VCBound
    p {BAD} ≤
# 七、VC维度
1.VCBound意义
2.dvc(H),满足mH(N) = 2^N的最大N，能shatter的最大H
4.对2D上PLA的VC维度
    dvc = d+1
5.物理意义
    W即H自由度，W无限则自由度无限，d+1表有效自由度，dvc表H强度、参数旋钮数、模型复杂度
7.Eout(g)置信区间，以及Eout、Ein、modelcomplexity图
8.vc维度的数据复杂度
    N ≈ 10dvc
# 八、噪音与错误衡量
3.错误衡量
    Ein、Eout逐点衡量公式
    方式一0/1、方式二square
4.错误接受、错误拒绝：混淆矩阵的假正（实际-1，预测+1）、假负（实际+1，预测-1）
5.错误衡量的算法
    自欺型0/1、square
    友善型
# 九、线性回归：通过线性回归H与errsqr计算WLIN=假逆阵Xy
2.对Ein微分解得Ein最小值WLIN=假逆阵Xy
4.Ein与帽子矩阵、保证Eout很小的方式——学习曲线图
6.err0/1与errsqr图——Eout ≤ Ein
    将WLin作为分类的基线可简化算法
# 十、逻辑回归：对交叉熵err(W,x,y)=ln(1+e^-wxy)梯度下降得最佳
1.soft二元分类
    用P(y|x)作目标函数f、用θ(W^T x)作假说
    逻辑函数（标准化转换函数:θ(s) = e^s / 1+e^s = 1 / 1+e^-s
    对假说h归一化得：h(x) = θ(W^T x) = 1 / 1+e^-W^T x
    设h为f，g=argmax h
3.最小化Ein = min 1/N ∑ln(1+ e^)
  逻辑回归的误差函数（交叉熵错误）err = ln(1+ e^-wxy)
4.梯度下降求 min w
    sign(Wx)是方向
    微分Ein = min 1/N ∑ln(1+ e^)
    固定学习率： Wt - n △Ein(Wt) → Wt+1 直到△Ein(Wt+1)≈0
# 十一、分类的线性model
1.三误差函数图的Eout上限
2.优化逻辑回归的速度。。。
3.元算法OVA
    对D逻辑回归得W，g取W的最大估计argmax
4.OVO
    对D二元分类，g取Wx的投票冠军
# 十二、非线性转变（二次假说）
1.核函数（非线函数）二次三项式h(x) = sign(w0z0 w1z1 w2z2)
  元转换：特征转换（圆圈转线）：（x， y） → （z， y）
  反转：令z = Φ(x)，h(z) = sign( wΦ(x) )
  二次假说Φ2(x) 与 高斯核转
3.代价：dvc↗
5.overfit与underfit
  overfit来自（dvc太高、噪音、N太小）
  高维转换伴随着高自由，同时伴随低Ein高Eout，模型应由简到易
# 十三、overfit的风险
1.二维学习曲线、十维（小N对高dvc的影响）
2.噪音对overfit的影响
    deterministic噪音、stochastic噪音
3.解决overfit
    从简单模型开始、数据清洗、数据增强、正则化、验证
# 十四、正则化
1.正则化H10
    思路：s.t. W3 = W4 = 。。。 = W10 = 0
    放松条件：得稀碎假说集H2'
    更放松条件：得正则假说集Hc
2.正则化的矩阵形式
    微分Ein(W)、设拉格朗日乘数、解λ与Wreg
    法二：解Ein(W)+λ/N WT W = Eaug(W)的最小值，加一点点λ=0.0001即可回正
    λ的意义：对向量W长度的惩罚
    正交化的基函数：勒詹德多项式
3.Eaug与Eout、Ein的关系
# 十五、验证
1.验证集Dval与Eval的由来
3.leave-one-out交叉验证的原理
# 十六：三原则
1.剃刀：线性模型优先、h务必从简
2.抽样偏差：test环境 ≈ train、注意数据的时间轴重要程度（重要数据加大权重）
3.数据窥探：站在别人的肩膀上，其实都是高dvc
  拇指原则：锁住test集、避免靠资料做决定、时刻怀疑数据的污染
